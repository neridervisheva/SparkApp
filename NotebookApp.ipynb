{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "satisfied-siemens",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "valued-henry",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "import sys, os\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
    "from pyspark.sql.functions import col, isnan, when, count, concat, lit, substring, udf, desc\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator, CrossValidatorModel\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opponent-printing",
   "metadata": {},
   "source": [
    "**Arguments from command line**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "elegant-librarian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Arguments passed: -f C:\\Users\\Ofeucor\\AppData\\Roaming\\jupyter\\runtime\\kernel-8e032ffb-8ad4-499e-bdf4-e2fe8fc27f96.json "
     ]
    }
   ],
   "source": [
    "print(\"\\nArguments passed:\", end = \" \")\n",
    "for i in range(1, len(sys.argv)):\n",
    "    print(sys.argv[i], end = \" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "built-activity",
   "metadata": {},
   "source": [
    "*python app.py --path ./Data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "exclusive-option",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add the argument \"--path\" followed by the address where the data is located.\n"
     ]
    }
   ],
   "source": [
    "if '--path' in sys.argv:\n",
    "    dir_path = sys.argv[sys.argv.index('--path')+1]\n",
    "else:\n",
    "    print('Add the argument \"--path\" followed by the address where the data is located.')\n",
    "    exit(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "available-greenhouse",
   "metadata": {},
   "source": [
    "**We create our app**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cleared-cotton",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-6TQUAIA:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>BigDataApp</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=BigDataApp>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"BigDataApp\").getOrCreate()\n",
    "spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developed-purchase",
   "metadata": {},
   "source": [
    "# LOADING THE DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emotional-banking",
   "metadata": {},
   "source": [
    "**Create a schema for the DataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "convertible-commonwealth",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"Year\",IntegerType(),nullable=True),\n",
    "    StructField(\"Month\",IntegerType(),nullable=True),\n",
    "    StructField(\"DayofMonth\",IntegerType(),nullable=True),\n",
    "    StructField(\"DayOfWeek\",IntegerType(),nullable=True),\n",
    "    StructField(\"DepTime\",IntegerType(),nullable=True),\n",
    "    StructField(\"CRSDepTime\",IntegerType(),nullable=True),\n",
    "    StructField(\"ArrTime\",IntegerType(),nullable=True),\n",
    "    StructField(\"CRSArrTime\",IntegerType(),nullable=True),\n",
    "    StructField(\"UniqueCarrier\",StringType(),nullable=True),\n",
    "    StructField(\"FlightNum\",IntegerType(),nullable=True),\n",
    "    StructField(\"TailNum\",StringType(),nullable=True),\n",
    "    StructField(\"ActualElapsedTime\",IntegerType(),nullable=True),\n",
    "    StructField(\"CRSElapsedTime\",IntegerType(),nullable=True),\n",
    "    StructField(\"AirTime\",IntegerType(),nullable=True),\n",
    "    StructField(\"ArrDelay\",IntegerType(),nullable=True),\n",
    "    StructField(\"DepDelay\",IntegerType(),nullable=True),\n",
    "    StructField(\"Origin\",StringType(),nullable=True),\n",
    "    StructField(\"Dest\",StringType(),nullable=True),\n",
    "    StructField(\"Distance\",IntegerType(),nullable=True),\n",
    "    StructField(\"TaxiIn\",IntegerType(),nullable=True),\n",
    "    StructField(\"TaxiOut\",IntegerType(),nullable=True),\n",
    "    StructField(\"Cancelled\",IntegerType(),nullable=True),\n",
    "    StructField(\"CancellationCode\",StringType(),nullable=True),\n",
    "    StructField(\"Diverted\",IntegerType(),nullable=True),\n",
    "    StructField(\"CarrierDelay\",IntegerType(),nullable=True),\n",
    "    StructField(\"WeatherDelay\",IntegerType(),nullable=True),\n",
    "    StructField(\"NASDelay\",IntegerType(),nullable=True),\n",
    "    StructField(\"SecurityDelay\",IntegerType(),nullable=True),\n",
    "    StructField(\"LateAircraftDelay\",IntegerType(),nullable=True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "substantial-favor",
   "metadata": {},
   "source": [
    "**Load data into DataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "sapphire-perry",
   "metadata": {},
   "outputs": [],
   "source": [
    "arg = './resources/data/2008.csv.bz2'\n",
    "dir_path = './resources/data'\n",
    "\n",
    "if dir_path[-1] != '/':\n",
    "    dir_path+='/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "brilliant-understanding",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./resources/data/2007.csv.bz2', './resources/data/2008.csv.bz2']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files_path = []\n",
    "for path in os.listdir(dir_path):\n",
    "    # check if current path is a file\n",
    "    if os.path.isfile(os.path.join(dir_path, path)) and path.endswith(\".csv.bz2\"):\n",
    "        files_path.append(dir_path+path)\n",
    "        \n",
    "files_path#df1.union(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "moral-register",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Year=2007, Month=1, DayofMonth=1, DayOfWeek=1, DepTime=1232, CRSDepTime=1225, ArrTime=1341, CRSArrTime=1340, UniqueCarrier='WN', FlightNum=2891, TailNum='N351', ActualElapsedTime=69, CRSElapsedTime=75, AirTime=54, ArrDelay=1, DepDelay=7, Origin='SMF', Dest='ONT', Distance=389, TaxiIn=4, TaxiOut=11, Cancelled=0, CancellationCode=None, Diverted=0, CarrierDelay=0, WeatherDelay=0, NASDelay=0, SecurityDelay=0, LateAircraftDelay=0),\n",
       " Row(Year=2007, Month=1, DayofMonth=1, DayOfWeek=1, DepTime=1918, CRSDepTime=1905, ArrTime=2043, CRSArrTime=2035, UniqueCarrier='WN', FlightNum=462, TailNum='N370', ActualElapsedTime=85, CRSElapsedTime=90, AirTime=74, ArrDelay=8, DepDelay=13, Origin='SMF', Dest='PDX', Distance=479, TaxiIn=5, TaxiOut=6, Cancelled=0, CancellationCode=None, Diverted=0, CarrierDelay=0, WeatherDelay=0, NASDelay=0, SecurityDelay=0, LateAircraftDelay=0),\n",
       " Row(Year=2007, Month=1, DayofMonth=1, DayOfWeek=1, DepTime=2206, CRSDepTime=2130, ArrTime=2334, CRSArrTime=2300, UniqueCarrier='WN', FlightNum=1229, TailNum='N685', ActualElapsedTime=88, CRSElapsedTime=90, AirTime=73, ArrDelay=34, DepDelay=36, Origin='SMF', Dest='PDX', Distance=479, TaxiIn=6, TaxiOut=9, Cancelled=0, CancellationCode=None, Diverted=0, CarrierDelay=3, WeatherDelay=0, NASDelay=0, SecurityDelay=0, LateAircraftDelay=31),\n",
       " Row(Year=2007, Month=1, DayofMonth=1, DayOfWeek=1, DepTime=1230, CRSDepTime=1200, ArrTime=1356, CRSArrTime=1330, UniqueCarrier='WN', FlightNum=1355, TailNum='N364', ActualElapsedTime=86, CRSElapsedTime=90, AirTime=75, ArrDelay=26, DepDelay=30, Origin='SMF', Dest='PDX', Distance=479, TaxiIn=3, TaxiOut=8, Cancelled=0, CancellationCode=None, Diverted=0, CarrierDelay=23, WeatherDelay=0, NASDelay=0, SecurityDelay=0, LateAircraftDelay=3),\n",
       " Row(Year=2007, Month=1, DayofMonth=1, DayOfWeek=1, DepTime=831, CRSDepTime=830, ArrTime=957, CRSArrTime=1000, UniqueCarrier='WN', FlightNum=2278, TailNum='N480', ActualElapsedTime=86, CRSElapsedTime=90, AirTime=74, ArrDelay=-3, DepDelay=1, Origin='SMF', Dest='PDX', Distance=479, TaxiIn=3, TaxiOut=9, Cancelled=0, CancellationCode=None, Diverted=0, CarrierDelay=0, WeatherDelay=0, NASDelay=0, SecurityDelay=0, LateAircraftDelay=0)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = spark.read.csv(path=files_path[0], schema=schema, header=True)\n",
    "\n",
    "for f in files_path[1:]:\n",
    "    df = df.union(spark.read.csv(path=f, schema=schema, header=True))\n",
    "display(df.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "private-session",
   "metadata": {},
   "source": [
    "**Remove forbidden variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "stopped-juvenile",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(\"ArrTime\").drop(\"ActualElapsedTime\"\n",
    "        ).drop(\"AirTime\").drop(\"TaxiIn\").drop(\"Diverted\"\n",
    "        ).drop(\"CarrierDelay\").drop(\"WeatherDelay\").drop(\"NASDelay\"\n",
    "        ).drop(\"SecurityDelay\").drop(\"LateAircraftDelay\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "polar-questionnaire",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- DayofMonth: integer (nullable = true)\n",
      " |-- DayOfWeek: integer (nullable = true)\n",
      " |-- DepTime: integer (nullable = true)\n",
      " |-- CRSDepTime: integer (nullable = true)\n",
      " |-- CRSArrTime: integer (nullable = true)\n",
      " |-- UniqueCarrier: string (nullable = true)\n",
      " |-- FlightNum: integer (nullable = true)\n",
      " |-- TailNum: string (nullable = true)\n",
      " |-- CRSElapsedTime: integer (nullable = true)\n",
      " |-- ArrDelay: integer (nullable = true)\n",
      " |-- DepDelay: integer (nullable = true)\n",
      " |-- Origin: string (nullable = true)\n",
      " |-- Dest: string (nullable = true)\n",
      " |-- Distance: integer (nullable = true)\n",
      " |-- TaxiOut: integer (nullable = true)\n",
      " |-- Cancelled: integer (nullable = true)\n",
      " |-- CancellationCode: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "environmental-chemical",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+---------+-------+----------+----------+-------------+---------+-------+--------------+--------+--------+------+----+--------+-------+---------+----------------+\n",
      "|Year|Month|DayofMonth|DayOfWeek|DepTime|CRSDepTime|CRSArrTime|UniqueCarrier|FlightNum|TailNum|CRSElapsedTime|ArrDelay|DepDelay|Origin|Dest|Distance|TaxiOut|Cancelled|CancellationCode|\n",
      "+----+-----+----------+---------+-------+----------+----------+-------------+---------+-------+--------------+--------+--------+------+----+--------+-------+---------+----------------+\n",
      "|2008|1    |3         |4        |1343   |1325      |1435      |WN           |588      |N240WN |70            |16      |18      |HOU   |LIT |393     |9      |0        |null            |\n",
      "|2008|1    |3         |4        |1125   |1120      |1245      |WN           |1343     |N523SW |85            |2       |5       |HOU   |MAF |441     |8      |0        |null            |\n",
      "|2008|1    |3         |4        |2009   |2015      |2140      |WN           |3841     |N280WN |85            |-4      |-6      |HOU   |MAF |441     |14     |0        |null            |\n",
      "|2008|1    |3         |4        |903    |855       |1205      |WN           |3        |N308SA |130           |-2      |8       |HOU   |MCO |848     |7      |0        |null            |\n",
      "|2008|1    |3         |4        |1423   |1400      |1710      |WN           |25       |N462WN |130           |16      |23      |HOU   |MCO |848     |10     |0        |null            |\n",
      "+----+-----+----------+---------+-------+----------+----------+-------------+---------+-------+--------------+--------+--------+------+----+--------+-------+---------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "saved-verification",
   "metadata": {},
   "source": [
    "# PROCESSING THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "latest-carnival",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2389217, 19)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df.count(), len(df.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quality-calculation",
   "metadata": {},
   "source": [
    "## Unique values of the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "alternative-advance",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "|Year|  count|\n",
      "+----+-------+\n",
      "|2008|2319115|\n",
      "+----+-------+\n",
      "\n",
      "None\n",
      "+-----+------+\n",
      "|Month| count|\n",
      "+-----+------+\n",
      "|    1|587130|\n",
      "|    3|598341|\n",
      "|    4|586723|\n",
      "|    2|546921|\n",
      "+-----+------+\n",
      "\n",
      "None\n",
      "+----------+-----+\n",
      "|DayofMonth|count|\n",
      "+----------+-----+\n",
      "|        31|38659|\n",
      "|        28|80203|\n",
      "|        27|77669|\n",
      "|        26|72013|\n",
      "|        12|71039|\n",
      "|        22|74352|\n",
      "|         1|74111|\n",
      "|        13|77147|\n",
      "|        16|75442|\n",
      "|         6|75778|\n",
      "|         3|78826|\n",
      "|        20|78537|\n",
      "|         5|72840|\n",
      "|        19|72256|\n",
      "|        15|76676|\n",
      "|        17|78076|\n",
      "|         9|73413|\n",
      "|         4|77589|\n",
      "|         8|74080|\n",
      "|        23|75512|\n",
      "+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n",
      "+---------+------+\n",
      "|DayOfWeek| count|\n",
      "+---------+------+\n",
      "|        1|337657|\n",
      "|        6|281129|\n",
      "|        3|354455|\n",
      "|        5|338154|\n",
      "|        4|340309|\n",
      "|        7|321369|\n",
      "|        2|346042|\n",
      "+---------+------+\n",
      "\n",
      "None\n",
      "+-------------+------+\n",
      "|UniqueCarrier| count|\n",
      "+-------------+------+\n",
      "|           UA|149156|\n",
      "|           AA|193114|\n",
      "|           NW|121162|\n",
      "|           EV| 89819|\n",
      "|           B6| 66381|\n",
      "|           DL|147987|\n",
      "|           OO|187917|\n",
      "|           F9| 30555|\n",
      "|           YV| 80363|\n",
      "|           AQ|  7752|\n",
      "|           US|150891|\n",
      "|           MQ|158426|\n",
      "|           OH| 69176|\n",
      "|           HA| 18262|\n",
      "|           XE|134444|\n",
      "|           AS| 48903|\n",
      "|           FL| 85708|\n",
      "|           CO|101394|\n",
      "|           WN|393732|\n",
      "|           9E| 83973|\n",
      "+-------------+------+\n",
      "\n",
      "None\n",
      "+---------+-----+\n",
      "|FlightNum|count|\n",
      "+---------+-----+\n",
      "|     5156|  194|\n",
      "|     4818|  261|\n",
      "|     3749|  557|\n",
      "|     7340|  108|\n",
      "|     3794|  284|\n",
      "|     1342|  203|\n",
      "|      833|  405|\n",
      "|     2122|  285|\n",
      "|      148|  956|\n",
      "|     1959|  113|\n",
      "|     4519|  180|\n",
      "|     1591|  624|\n",
      "|      463|  831|\n",
      "|     1580|  858|\n",
      "|     2866|  251|\n",
      "|     6620|  157|\n",
      "|     7240|   95|\n",
      "|      496|  691|\n",
      "|     1088|  546|\n",
      "|     3918|  230|\n",
      "+---------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n",
      "+-------+-----+\n",
      "|TailNum|count|\n",
      "+-------+-----+\n",
      "| N499AA|  446|\n",
      "| N672SW|  815|\n",
      "| N686AE|  712|\n",
      "| N866AS|  599|\n",
      "| N466SW|  698|\n",
      "| N919UA|  544|\n",
      "| 89709E|  568|\n",
      "| N902DE|  538|\n",
      "| N102UW|  359|\n",
      "| N407AA|  432|\n",
      "| N4YUAA|  375|\n",
      "| N516UA|  334|\n",
      "| N33637|  319|\n",
      "| N567AA|  368|\n",
      "| N369NB|  457|\n",
      "| N331NB|  471|\n",
      "| N73283|  312|\n",
      "|  N6700|  306|\n",
      "| N3CWAA|  308|\n",
      "| N388DA|  299|\n",
      "+-------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n",
      "+--------------+-----+\n",
      "|CRSElapsedTime|count|\n",
      "+--------------+-----+\n",
      "|           148| 6547|\n",
      "|           471|   56|\n",
      "|           243| 1225|\n",
      "|           392|  254|\n",
      "|            31|  347|\n",
      "|            85|54110|\n",
      "|           137| 7296|\n",
      "|           251| 2088|\n",
      "|            65|58525|\n",
      "|           458|   54|\n",
      "|            53| 5123|\n",
      "|           255| 3917|\n",
      "|           481|   30|\n",
      "|           133| 7220|\n",
      "|           296|  826|\n",
      "|            78|13483|\n",
      "|           322| 1039|\n",
      "|           513|    6|\n",
      "|           321|  744|\n",
      "|           362|  356|\n",
      "+--------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n",
      "+--------+-----+\n",
      "|ArrDelay|count|\n",
      "+--------+-----+\n",
      "|     148|  584|\n",
      "|     833|    1|\n",
      "|     471|    2|\n",
      "|     -35| 1606|\n",
      "|     243|  108|\n",
      "|     540|    4|\n",
      "|     392|   16|\n",
      "|      31| 9539|\n",
      "|     516|    6|\n",
      "|     137|  699|\n",
      "|      85| 1999|\n",
      "|     251|  110|\n",
      "|     451|    5|\n",
      "|      65| 3137|\n",
      "|      53| 4278|\n",
      "|     255|   90|\n",
      "|     133|  735|\n",
      "|     322|   35|\n",
      "|      78| 2317|\n",
      "|     362|   24|\n",
      "+--------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n",
      "+--------+-----+\n",
      "|DepDelay|count|\n",
      "+--------+-----+\n",
      "|     148|  501|\n",
      "|     463|    7|\n",
      "|     243|  106|\n",
      "|     392|    5|\n",
      "|     897|    2|\n",
      "|     -35|   16|\n",
      "|      31| 7431|\n",
      "|      85| 1970|\n",
      "|     137|  631|\n",
      "|     251|   84|\n",
      "|     580|    2|\n",
      "|     451|    5|\n",
      "|      65| 3281|\n",
      "|     458|    4|\n",
      "|      53| 3743|\n",
      "|     255|   99|\n",
      "|     133|  654|\n",
      "|     296|   47|\n",
      "|     472|    3|\n",
      "|      78| 1967|\n",
      "+--------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n",
      "+------+-----+\n",
      "|Origin|count|\n",
      "+------+-----+\n",
      "|   BGM|  227|\n",
      "|   PSE|  299|\n",
      "|   MSY|13587|\n",
      "|   GEG| 5327|\n",
      "|   SNA|16006|\n",
      "|   BUR|10512|\n",
      "|   GRB| 2577|\n",
      "|   GTF|  734|\n",
      "|   IDA| 1113|\n",
      "|   GRR| 5145|\n",
      "|   PSG|  226|\n",
      "|   EUG| 2032|\n",
      "|   MYR| 1557|\n",
      "|   PVD| 7574|\n",
      "|   GSO| 4389|\n",
      "|   OAK|22136|\n",
      "|   COD|  356|\n",
      "|   FSM|  888|\n",
      "|   MQT|  301|\n",
      "|   MSN| 4052|\n",
      "+------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n",
      "+----+-----+\n",
      "|Dest|count|\n",
      "+----+-----+\n",
      "| BGM|  228|\n",
      "| DLG|    2|\n",
      "| PSE|  299|\n",
      "| MSY|13613|\n",
      "| GEG| 5332|\n",
      "| BUR|10574|\n",
      "| SNA|16049|\n",
      "| GRB| 2596|\n",
      "| GTF|  736|\n",
      "| IDA| 1118|\n",
      "| GRR| 5175|\n",
      "| PSG|  230|\n",
      "| EUG| 2032|\n",
      "| PVD| 7598|\n",
      "| GSO| 4427|\n",
      "| MYR| 1567|\n",
      "| OAK|22163|\n",
      "| MQT|  301|\n",
      "| COD|  356|\n",
      "| MSN| 4078|\n",
      "+----+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n",
      "+--------+-----+\n",
      "|Distance|count|\n",
      "+--------+-----+\n",
      "|     463|  752|\n",
      "|     496| 1384|\n",
      "|     833| 1824|\n",
      "|     471|  599|\n",
      "|     148| 1562|\n",
      "|    1342|  715|\n",
      "|    1959|  241|\n",
      "|    1088|  712|\n",
      "|    1591| 1048|\n",
      "|    1829| 1022|\n",
      "|     392|  938|\n",
      "|     243|  719|\n",
      "|     737| 1888|\n",
      "|     540|  960|\n",
      "|    1084|  625|\n",
      "|     623|  633|\n",
      "|    1721| 2956|\n",
      "|    1522| 1624|\n",
      "|    1025|  597|\n",
      "|     897|  127|\n",
      "+--------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n",
      "+-------+-----+\n",
      "|TaxiOut|count|\n",
      "+-------+-----+\n",
      "|    148|   17|\n",
      "|     31|15403|\n",
      "|     85|  287|\n",
      "|    137|   33|\n",
      "|     65|  848|\n",
      "|     53| 1977|\n",
      "|    133|   44|\n",
      "|     78|  409|\n",
      "|    108|  110|\n",
      "|    155|   13|\n",
      "|     34|11115|\n",
      "|    101|  126|\n",
      "|    115|   76|\n",
      "|    126|   37|\n",
      "|     81|  349|\n",
      "|     28|22031|\n",
      "|    183|    4|\n",
      "|     76|  431|\n",
      "|     26|28711|\n",
      "|     27|25286|\n",
      "+-------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n",
      "+----------+-----+\n",
      "|      Date|count|\n",
      "+----------+-----+\n",
      "|04/05/2008|16939|\n",
      "|03/28/2008|20534|\n",
      "|04/24/2008|20524|\n",
      "|01/15/2008|19167|\n",
      "|01/08/2008|19104|\n",
      "|03/29/2008|17168|\n",
      "|03/02/2008|19095|\n",
      "|03/14/2008|20591|\n",
      "|04/13/2008|19442|\n",
      "|01/13/2008|18549|\n",
      "|03/27/2008|19997|\n",
      "|02/10/2008|18438|\n",
      "|04/23/2008|20320|\n",
      "|03/11/2008|19882|\n",
      "|02/05/2008|18322|\n",
      "|01/24/2008|19899|\n",
      "|04/30/2008|20311|\n",
      "|03/08/2008|15670|\n",
      "|03/23/2008|19309|\n",
      "|03/25/2008|19971|\n",
      "+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n",
      "+------+-------+\n",
      "|Season|  count|\n",
      "+------+-------+\n",
      "|     1|1185064|\n",
      "|     4|1134051|\n",
      "+------+-------+\n",
      "\n",
      "None\n",
      "+----------------+-----+\n",
      "|        datetime|count|\n",
      "+----------------+-----+\n",
      "|23/01/2008 10:50|   74|\n",
      "|05/01/2008 12:45|   84|\n",
      "|06/02/2008 15:55|   57|\n",
      "|13/02/2008 18:05|   62|\n",
      "|27/04/2008 20:35|   51|\n",
      "|17/03/2008 05:45|   26|\n",
      "|18/03/2008 07:00|  280|\n",
      "|29/01/2008 08:20|   86|\n",
      "|06/01/2008 08:40|   85|\n",
      "|19/01/2008 09:25|   76|\n",
      "|06/02/2008 09:30|  105|\n",
      "|06/03/2008 10:00|  102|\n",
      "|08/02/2008 10:15|   83|\n",
      "|24/02/2008 12:00|  105|\n",
      "|25/04/2008 13:40|   73|\n",
      "|24/02/2008 16:15|   95|\n",
      "|15/04/2008 17:55|   77|\n",
      "|17/04/2008 06:00|  423|\n",
      "|24/04/2008 06:55|   61|\n",
      "|02/04/2008 07:20|   88|\n",
      "+----------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n",
      "+--------+-----+\n",
      "|Duration|count|\n",
      "+--------+-----+\n",
      "|     148| 6014|\n",
      "|     496|   63|\n",
      "|    -946|  321|\n",
      "|     463|  229|\n",
      "|     471|  236|\n",
      "|     392|  376|\n",
      "|    -975|  560|\n",
      "|     243|  812|\n",
      "|    -967|  115|\n",
      "|   -1302|   40|\n",
      "|      31| 1974|\n",
      "|      85|51534|\n",
      "|     137| 4994|\n",
      "|     251|  412|\n",
      "|     451|  197|\n",
      "|   -1069|   56|\n",
      "|    -949|   46|\n",
      "|      65|56489|\n",
      "|     458|  270|\n",
      "|    -987|  140|\n",
      "+--------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "for c in df.columns:\n",
    "    print(df.groupBy(c).count().show())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yellow-patio",
   "metadata": {},
   "source": [
    "## Removing noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "detailed-basement",
   "metadata": {},
   "source": [
    "**Remove duplicated rows**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cooked-applicant",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "innocent-regulation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+---------+-------+----------+----------+-------------+---------+-------+--------------+--------+--------+------+----+--------+-------+---------+----------------+-----+\n",
      "|Year|Month|DayofMonth|DayOfWeek|DepTime|CRSDepTime|CRSArrTime|UniqueCarrier|FlightNum|TailNum|CRSElapsedTime|ArrDelay|DepDelay|Origin|Dest|Distance|TaxiOut|Cancelled|CancellationCode|count|\n",
      "+----+-----+----------+---------+-------+----------+----------+-------------+---------+-------+--------------+--------+--------+------+----+--------+-------+---------+----------------+-----+\n",
      "+----+-----+----------+---------+-------+----------+----------+-------------+---------+-------+--------------+--------+--------+------+----+--------+-------+---------+----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(df.columns).count().filter(\"count > 1\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "partial-bangladesh",
   "metadata": {},
   "source": [
    "**Remove instances of cancelled flights**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "painful-granny",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+\n",
      "|Cancelled|  count|\n",
      "+---------+-------+\n",
      "|        1|  64442|\n",
      "|        0|2324771|\n",
      "+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('Cancelled').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "insured-detector",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------+\n",
      "|CancellationCode|  count|\n",
      "+----------------+-------+\n",
      "|            null|2324771|\n",
      "|               B|  25744|\n",
      "|               C|  12617|\n",
      "|               A|  26075|\n",
      "|               D|      6|\n",
      "+----------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('CancellationCode').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "medical-banks",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.filter(df.Cancelled == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "naked-compilation",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.filter(df.CancellationCode.isNull())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handy-shepherd",
   "metadata": {},
   "source": [
    "**CancellationCode has more than 97% missing so it is removed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "crazy-remedy",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('CancellationCode', 'Cancelled')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "immune-liberal",
   "metadata": {},
   "source": [
    "**Analyze missing values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "regional-century",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name                Count %\n",
      "--------------  -----------\n",
      "Year            0\n",
      "Month           0\n",
      "DayofMonth      0\n",
      "DayOfWeek       0\n",
      "DepTime         0\n",
      "CRSDepTime      0\n",
      "CRSArrTime      0\n",
      "UniqueCarrier   0\n",
      "FlightNum       0\n",
      "TailNum         1.29045e-06\n",
      "CRSElapsedTime  0.000123883\n",
      "ArrDelay        0.00243207\n",
      "DepDelay        0\n",
      "Origin          0\n",
      "Dest            0\n",
      "Distance        0\n",
      "TaxiOut         0\n"
     ]
    }
   ],
   "source": [
    "print(tabulate([[c, df.filter(col(c).isNull()).count()/df.count()] for c in df.columns], headers=['Name', 'Count %']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "severe-japanese",
   "metadata": {},
   "source": [
    "**TailNum and CRSElapsedTime can not be imputed from any other column, and ArrDelay is the target variable, but their number of missing values is not significant taking into account the total number, so the rows containing those missing values are removed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bridal-classification",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "adjacent-allah",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2319115, 17)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df.count(), len(df.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occupied-moscow",
   "metadata": {},
   "source": [
    "## !!!!!Date preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "crude-vessel",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|      Date|count|\n",
      "+----------+-----+\n",
      "|04/05/2008|16939|\n",
      "|03/28/2008|20534|\n",
      "|04/24/2008|20524|\n",
      "|01/15/2008|19167|\n",
      "|01/08/2008|19104|\n",
      "|03/29/2008|17168|\n",
      "|03/02/2008|19095|\n",
      "|03/14/2008|20591|\n",
      "|04/13/2008|19442|\n",
      "|01/13/2008|18549|\n",
      "|03/27/2008|19997|\n",
      "|02/10/2008|18438|\n",
      "|04/23/2008|20320|\n",
      "|03/11/2008|19882|\n",
      "|02/05/2008|18322|\n",
      "|01/24/2008|19899|\n",
      "|04/30/2008|20311|\n",
      "|03/08/2008|15670|\n",
      "|03/23/2008|19309|\n",
      "|03/25/2008|19971|\n",
      "+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\n",
    "    \"Date\", F.date_format(F.expr(\"make_date(Year, Month, DayofMonth)\"), \"MM/dd/yyyy\")\n",
    ")\n",
    "\n",
    "df.groupBy('Date').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "contemporary-bracket",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+\n",
      "|Season|  count|\n",
      "+------+-------+\n",
      "|     1|1185064|\n",
      "|     4|1134051|\n",
      "+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\n",
    "    \"Season\", when((df.Month>2) & (df.Month<6), 1).when((df.Month>5) & (df.Month<9), 2\n",
    "        ).when((df.Month>8) & (df.Month<12), 3).otherwise(4)\n",
    ")\n",
    "df.groupBy('Season').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brazilian-ethics",
   "metadata": {},
   "source": [
    "**Check that DepDelay = DepTime - CRSDepTime**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "square-clear",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"DepTimeNew\", when(F.length(df.DepTime) == 3, concat(lit(\"0\"),df.DepTime)) \\\n",
    "        .when(F.length(df.DepTime) == 2, concat(lit(\"00\"),df.DepTime)) \\\n",
    "        .otherwise(df.DepTime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "awful-virginia",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"CRSDepTimeNew\", when(F.length(df.CRSDepTime) == 3, concat(lit(\"0\"),df.CRSDepTime)) \\\n",
    "        .when(F.length(df.CRSDepTime) == 2, concat(lit(\"00\"),df.CRSDepTime)) \\\n",
    "        .otherwise(df.CRSDepTime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "sudden-dress",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"CRSArrTimeNew\", when(F.length(df.CRSArrTime) == 3, concat(lit(\"0\"),df.CRSArrTime)) \\\n",
    "        .when(F.length(df.CRSArrTime) == 2, concat(lit(\"00\"),df.CRSArrTime)) \\\n",
    "        .otherwise(df.CRSArrTime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "caroline-stereo",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----------+-------------+----------+-------------+\n",
      "|DepTime|DepTimeNew|CRSDepTime|CRSDepTimeNew|CRSArrTime|CRSArrTimeNew|\n",
      "+-------+----------+----------+-------------+----------+-------------+\n",
      "|   1215|      1215|      1220|         1220|      1324|         1324|\n",
      "|   1540|      1540|      1409|         1409|      1711|         1711|\n",
      "|   1835|      1835|      1755|         1755|      1845|         1845|\n",
      "|    709|      0709|       655|         0655|       830|         0830|\n",
      "|    700|      0700|       705|         0705|       825|         0825|\n",
      "|    724|      0724|       737|         0737|       859|         0859|\n",
      "|    738|      0738|       740|         0740|       835|         0835|\n",
      "|    742|      0742|       745|         0745|       845|         0845|\n",
      "|    852|      0852|       850|         0850|      1105|         1105|\n",
      "|    905|      0905|       908|         0908|      1015|         1015|\n",
      "|    930|      0930|       935|         0935|      1030|         1030|\n",
      "|    940|      0940|       940|         0940|      1105|         1105|\n",
      "|   1029|      1029|      1010|         1010|      1115|         1115|\n",
      "|   1037|      1037|      1040|         1040|      1140|         1140|\n",
      "|   1051|      1051|      1040|         1040|      1140|         1140|\n",
      "|   1239|      1239|      1136|         1136|      1245|         1245|\n",
      "|   1259|      1259|      1300|         1300|      1415|         1415|\n",
      "|   1325|      1325|      1315|         1315|      1512|         1512|\n",
      "|   1437|      1437|      1425|         1425|      1650|         1650|\n",
      "|   1435|      1435|      1436|         1436|      1516|         1516|\n",
      "+-------+----------+----------+-------------+----------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"DepTime\",\"DepTimeNew\",\"CRSDepTime\",\"CRSDepTimeNew\",\"CRSArrTime\",\"CRSArrTimeNew\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "enabling-valley",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+--------+\n",
      "|DepTimeNew|CRSDepTimeNew|DepDelay|\n",
      "+----------+-------------+--------+\n",
      "|      1215|         1220|      -5|\n",
      "|      1540|         1409|      91|\n",
      "|      1835|         1755|      40|\n",
      "|      0709|         0655|      14|\n",
      "|      0700|         0705|      -5|\n",
      "|      0724|         0737|     -13|\n",
      "|      0738|         0740|      -2|\n",
      "|      0742|         0745|      -3|\n",
      "|      0852|         0850|       2|\n",
      "|      0905|         0908|      -3|\n",
      "|      0930|         0935|      -5|\n",
      "|      0940|         0940|       0|\n",
      "|      1029|         1010|      19|\n",
      "|      1037|         1040|      -3|\n",
      "|      1051|         1040|      11|\n",
      "|      1239|         1136|      63|\n",
      "|      1259|         1300|      -1|\n",
      "|      1325|         1315|      10|\n",
      "|      1437|         1425|      12|\n",
      "|      1435|         1436|      -1|\n",
      "+----------+-------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"DepTimeNew\",\"CRSDepTimeNew\",\"DepDelay\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "approximate-brazil",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"CRSDepTimeNewHour\", substring(df.CRSDepTimeNew, 1,2)) \\\n",
    "    .withColumn(\"CRSDepTimeNewMinute\", substring(df.CRSDepTimeNew, 3,2)) \\\n",
    "    .withColumn(\"DepTimeNewHour\", substring(df.DepTimeNew, 1,2)) \\\n",
    "    .withColumn(\"DepTimeNewMinute\", substring(df.DepTimeNew, 3,2))\n",
    "\n",
    "df = df.withColumn(\"CRSArrTimeNewHour\", substring(df.CRSArrTimeNew, 1,2)) \\\n",
    "    .withColumn(\"CRSArrTimeNewMinute\", substring(df.CRSArrTimeNew, 3,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "intermediate-collector",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------------+-------------------+----------+--------------+----------------+\n",
      "|CRSDepTimeNew|CRSDepTimeNewHour|CRSDepTimeNewMinute|DepTimeNew|DepTimeNewHour|DepTimeNewMinute|\n",
      "+-------------+-----------------+-------------------+----------+--------------+----------------+\n",
      "|         1220|               12|                 20|      1215|            12|              15|\n",
      "|         1409|               14|                 09|      1540|            15|              40|\n",
      "|         1755|               17|                 55|      1835|            18|              35|\n",
      "|         0655|               06|                 55|      0709|            07|              09|\n",
      "|         0705|               07|                 05|      0700|            07|              00|\n",
      "|         0737|               07|                 37|      0724|            07|              24|\n",
      "|         0740|               07|                 40|      0738|            07|              38|\n",
      "|         0745|               07|                 45|      0742|            07|              42|\n",
      "|         0850|               08|                 50|      0852|            08|              52|\n",
      "|         0908|               09|                 08|      0905|            09|              05|\n",
      "|         0935|               09|                 35|      0930|            09|              30|\n",
      "|         0940|               09|                 40|      0940|            09|              40|\n",
      "|         1010|               10|                 10|      1029|            10|              29|\n",
      "|         1040|               10|                 40|      1037|            10|              37|\n",
      "|         1040|               10|                 40|      1051|            10|              51|\n",
      "|         1136|               11|                 36|      1239|            12|              39|\n",
      "|         1300|               13|                 00|      1259|            12|              59|\n",
      "|         1315|               13|                 15|      1325|            13|              25|\n",
      "|         1425|               14|                 25|      1437|            14|              37|\n",
      "|         1436|               14|                 36|      1435|            14|              35|\n",
      "+-------------+-----------------+-------------------+----------+--------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"CRSDepTimeNew\",\"CRSDepTimeNewHour\",\"CRSDepTimeNewMinute\",\"DepTimeNew\",\"DepTimeNewHour\",\"DepTimeNewMinute\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "functional-animal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|        datetime|\n",
      "+----------------+\n",
      "|28/01/2008 12:20|\n",
      "|27/01/2008 14:09|\n",
      "|04/01/2008 17:55|\n",
      "|21/01/2008 06:55|\n",
      "|21/01/2008 07:05|\n",
      "|16/01/2008 07:37|\n",
      "|02/01/2008 07:40|\n",
      "|29/01/2008 07:45|\n",
      "|07/01/2008 08:50|\n",
      "|30/01/2008 09:08|\n",
      "|20/01/2008 09:35|\n",
      "|20/01/2008 09:40|\n",
      "|28/01/2008 10:10|\n",
      "|11/01/2008 10:40|\n",
      "|25/01/2008 10:40|\n",
      "|27/01/2008 11:36|\n",
      "|30/01/2008 13:00|\n",
      "|23/01/2008 13:15|\n",
      "|05/01/2008 14:25|\n",
      "|15/01/2008 14:36|\n",
      "+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\n",
    "    \"datetime\",\n",
    "    F.date_format(\n",
    "        F.expr(\"make_timestamp(Year, Month, DayofMonth, CRSDepTimeNewHour, CRSDepTimeNewMinute, 0)\"),\n",
    "        \"dd/MM/yyyy HH:mm\"\n",
    "    )\n",
    ")\n",
    "\n",
    "df.select(\"datetime\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "governing-mistress",
   "metadata": {},
   "source": [
    "#   !!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "hearing-cross",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Column 'CRSArrTimeNewHour' does not exist. Did you mean one of the following? [ArrDelay, CRSElapsedTime, Duration, Season, TailNum, DayOfWeek, FlightNum, Origin, TaxiOut, UniqueCarrier, Year, DepDelay, Dest, Distance, DayofMonth, Month];\n'Project [Year#0, Month#1, DayofMonth#2, DayOfWeek#3, UniqueCarrier#8, FlightNum#9, TailNum#10, CRSElapsedTime#12, ArrDelay#14, DepDelay#15, Origin#16, Dest#17, Distance#18, TaxiOut#20, Season#767, (((cast('CRSArrTimeNewHour as bigint) * 60) + cast('CRSArrTimeNewMinute as bigint)) - ((cast('CRSDepTimeNewHour as bigint) * 60) + cast('CRSDepTimeNewMinute as bigint))) AS Duration#2219]\n+- Project [Year#0, Month#1, DayofMonth#2, DayOfWeek#3, UniqueCarrier#8, FlightNum#9, TailNum#10, CRSElapsedTime#12, ArrDelay#14, DepDelay#15, Origin#16, Dest#17, Distance#18, TaxiOut#20, Season#767, Duration#1218]\n   +- Project [Year#0, Month#1, DayofMonth#2, DayOfWeek#3, DepTime#4, CRSDepTime#5, CRSArrTime#7, UniqueCarrier#8, FlightNum#9, TailNum#10, CRSElapsedTime#12, ArrDelay#14, DepDelay#15, Origin#16, Dest#17, Distance#18, TaxiOut#20, Season#767, DepTimeNew#838, CRSDepTimeNew#858, CRSArrTimeNew#879, CRSDepTimeNewHour#986, CRSDepTimeNewMinute#1009, DepTimeNewHour#1033, ... 4 more fields]\n      +- Project [Year#0, Month#1, DayofMonth#2, DayOfWeek#3, DepTime#4, CRSDepTime#5, CRSArrTime#7, UniqueCarrier#8, FlightNum#9, TailNum#10, CRSElapsedTime#12, ArrDelay#14, DepDelay#15, Origin#16, Dest#17, Distance#18, TaxiOut#20, Season#767, DepTimeNew#838, CRSDepTimeNew#858, CRSArrTimeNew#879, CRSDepTimeNewHour#986, CRSDepTimeNewMinute#1009, DepTimeNewHour#1033, ... 4 more fields]\n         +- Project [Year#0, Month#1, DayofMonth#2, DayOfWeek#3, DepTime#4, CRSDepTime#5, CRSArrTime#7, UniqueCarrier#8, FlightNum#9, TailNum#10, CRSElapsedTime#12, ArrDelay#14, DepDelay#15, Origin#16, Dest#17, Distance#18, TaxiOut#20, Season#767, DepTimeNew#838, CRSDepTimeNew#858, CRSArrTimeNew#879, CRSDepTimeNewHour#986, CRSDepTimeNewMinute#1009, DepTimeNewHour#1033, ... 3 more fields]\n            +- Project [Year#0, Month#1, DayofMonth#2, DayOfWeek#3, DepTime#4, CRSDepTime#5, CRSArrTime#7, UniqueCarrier#8, FlightNum#9, TailNum#10, CRSElapsedTime#12, ArrDelay#14, DepDelay#15, Origin#16, Dest#17, Distance#18, TaxiOut#20, Season#767, DepTimeNew#838, CRSDepTimeNew#858, CRSArrTimeNew#879, CRSDepTimeNewHour#986, CRSDepTimeNewMinute#1009, DepTimeNewHour#1033, ... 2 more fields]\n               +- Project [Year#0, Month#1, DayofMonth#2, DayOfWeek#3, DepTime#4, CRSDepTime#5, CRSArrTime#7, UniqueCarrier#8, FlightNum#9, TailNum#10, CRSElapsedTime#12, ArrDelay#14, DepDelay#15, Origin#16, Dest#17, Distance#18, TaxiOut#20, Season#767, DepTimeNew#838, CRSDepTimeNew#858, CRSArrTimeNew#879, CRSDepTimeNewHour#986, CRSDepTimeNewMinute#1009, DepTimeNewHour#1033, substring(DepTimeNew#838, 3, 2) AS DepTimeNewMinute#1058]\n                  +- Project [Year#0, Month#1, DayofMonth#2, DayOfWeek#3, DepTime#4, CRSDepTime#5, CRSArrTime#7, UniqueCarrier#8, FlightNum#9, TailNum#10, CRSElapsedTime#12, ArrDelay#14, DepDelay#15, Origin#16, Dest#17, Distance#18, TaxiOut#20, Season#767, DepTimeNew#838, CRSDepTimeNew#858, CRSArrTimeNew#879, CRSDepTimeNewHour#986, CRSDepTimeNewMinute#1009, substring(DepTimeNew#838, 1, 2) AS DepTimeNewHour#1033]\n                     +- Project [Year#0, Month#1, DayofMonth#2, DayOfWeek#3, DepTime#4, CRSDepTime#5, CRSArrTime#7, UniqueCarrier#8, FlightNum#9, TailNum#10, CRSElapsedTime#12, ArrDelay#14, DepDelay#15, Origin#16, Dest#17, Distance#18, TaxiOut#20, Season#767, DepTimeNew#838, CRSDepTimeNew#858, CRSArrTimeNew#879, CRSDepTimeNewHour#986, substring(CRSDepTimeNew#858, 3, 2) AS CRSDepTimeNewMinute#1009]\n                        +- Project [Year#0, Month#1, DayofMonth#2, DayOfWeek#3, DepTime#4, CRSDepTime#5, CRSArrTime#7, UniqueCarrier#8, FlightNum#9, TailNum#10, CRSElapsedTime#12, ArrDelay#14, DepDelay#15, Origin#16, Dest#17, Distance#18, TaxiOut#20, Season#767, DepTimeNew#838, CRSDepTimeNew#858, CRSArrTimeNew#879, substring(CRSDepTimeNew#858, 1, 2) AS CRSDepTimeNewHour#986]\n                           +- Project [Year#0, Month#1, DayofMonth#2, DayOfWeek#3, DepTime#4, CRSDepTime#5, CRSArrTime#7, UniqueCarrier#8, FlightNum#9, TailNum#10, CRSElapsedTime#12, ArrDelay#14, DepDelay#15, Origin#16, Dest#17, Distance#18, TaxiOut#20, Season#767, DepTimeNew#838, CRSDepTimeNew#858, CASE WHEN (length(cast(CRSArrTime#7 as string)) = 3) THEN concat(0, cast(CRSArrTime#7 as string)) WHEN (length(cast(CRSArrTime#7 as string)) = 2) THEN concat(00, cast(CRSArrTime#7 as string)) ELSE cast(CRSArrTime#7 as string) END AS CRSArrTimeNew#879]\n                              +- Project [Year#0, Month#1, DayofMonth#2, DayOfWeek#3, DepTime#4, CRSDepTime#5, CRSArrTime#7, UniqueCarrier#8, FlightNum#9, TailNum#10, CRSElapsedTime#12, ArrDelay#14, DepDelay#15, Origin#16, Dest#17, Distance#18, TaxiOut#20, Season#767, DepTimeNew#838, CASE WHEN (length(cast(CRSDepTime#5 as string)) = 3) THEN concat(0, cast(CRSDepTime#5 as string)) WHEN (length(cast(CRSDepTime#5 as string)) = 2) THEN concat(00, cast(CRSDepTime#5 as string)) ELSE cast(CRSDepTime#5 as string) END AS CRSDepTimeNew#858]\n                                 +- Project [Year#0, Month#1, DayofMonth#2, DayOfWeek#3, DepTime#4, CRSDepTime#5, CRSArrTime#7, UniqueCarrier#8, FlightNum#9, TailNum#10, CRSElapsedTime#12, ArrDelay#14, DepDelay#15, Origin#16, Dest#17, Distance#18, TaxiOut#20, Season#767, CASE WHEN (length(cast(DepTime#4 as string)) = 3) THEN concat(0, cast(DepTime#4 as string)) WHEN (length(cast(DepTime#4 as string)) = 2) THEN concat(00, cast(DepTime#4 as string)) ELSE cast(DepTime#4 as string) END AS DepTimeNew#838]\n                                    +- Project [Year#0, Month#1, DayofMonth#2, DayOfWeek#3, DepTime#4, CRSDepTime#5, CRSArrTime#7, UniqueCarrier#8, FlightNum#9, TailNum#10, CRSElapsedTime#12, ArrDelay#14, DepDelay#15, Origin#16, Dest#17, Distance#18, TaxiOut#20, CASE WHEN ((Month#1 > 2) AND (Month#1 < 6)) THEN 1 WHEN ((Month#1 > 5) AND (Month#1 < 9)) THEN 2 WHEN ((Month#1 > 8) AND (Month#1 < 12)) THEN 3 ELSE 4 END AS Season#767]\n                                       +- Filter atleastnnonnulls(17, Year#0, Month#1, DayofMonth#2, DayOfWeek#3, DepTime#4, CRSDepTime#5, CRSArrTime#7, UniqueCarrier#8, FlightNum#9, TailNum#10, CRSElapsedTime#12, ArrDelay#14, DepDelay#15, Origin#16, Dest#17, Distance#18, TaxiOut#20)\n                                          +- Project [Year#0, Month#1, DayofMonth#2, DayOfWeek#3, DepTime#4, CRSDepTime#5, CRSArrTime#7, UniqueCarrier#8, FlightNum#9, TailNum#10, CRSElapsedTime#12, ArrDelay#14, DepDelay#15, Origin#16, Dest#17, Distance#18, TaxiOut#20]\n                                             +- Filter isnull(CancellationCode#22)\n                                                +- Filter (Cancelled#21 = 0)\n                                                   +- Deduplicate [TaxiOut#20, CRSDepTime#5, Month#1, Dest#17, Origin#16, DepTime#4, DayOfWeek#3, Year#0, UniqueCarrier#8, ArrDelay#14, Distance#18, DepDelay#15, FlightNum#9, CRSElapsedTime#12, CancellationCode#22, DayofMonth#2, CRSArrTime#7, TailNum#10, Cancelled#21]\n                                                      +- Project [Year#0, Month#1, DayofMonth#2, DayOfWeek#3, DepTime#4, CRSDepTime#5, CRSArrTime#7, UniqueCarrier#8, FlightNum#9, TailNum#10, CRSElapsedTime#12, ArrDelay#14, DepDelay#15, Origin#16, Dest#17, Distance#18, TaxiOut#20, Cancelled#21, CancellationCode#22]\n                                                         +- Project [Year#0, Month#1, DayofMonth#2, DayOfWeek#3, DepTime#4, CRSDepTime#5, CRSArrTime#7, UniqueCarrier#8, FlightNum#9, TailNum#10, CRSElapsedTime#12, ArrDelay#14, DepDelay#15, Origin#16, Dest#17, Distance#18, TaxiOut#20, Cancelled#21, CancellationCode#22, LateAircraftDelay#28]\n                                                            +- Project [Year#0, Month#1, DayofMonth#2, DayOfWeek#3, DepTime#4, CRSDepTime#5, CRSArrTime#7, UniqueCarrier#8, FlightNum#9, TailNum#10, CRSElapsedTime#12, ArrDelay#14, DepDelay#15, Origin#16, Dest#17, Distance#18, TaxiOut#20, Cancelled#21, CancellationCode#22, SecurityDelay#27, LateAircraftDelay#28]\n                                                               +- Project [Year#0, Month#1, DayofMonth#2, DayOfWeek#3, DepTime#4, CRSDepTime#5, CRSArrTime#7, UniqueCarrier#8, FlightNum#9, TailNum#10, CRSElapsedTime#12, ArrDelay#14, DepDelay#15, Origin#16, Dest#17, Distance#18, TaxiOut#20, Cancelled#21, CancellationCode#22, NASDelay#26, SecurityDelay#27, LateAircraftDelay#28]\n                                                                  +- Project [Year#0, Month#1, DayofMonth#2, DayOfWeek#3, DepTime#4, CRSDepTime#5, CRSArrTime#7, UniqueCarrier#8, FlightNum#9, TailNum#10, CRSElapsedTime#12, ArrDelay#14, DepDelay#15, Origin#16, Dest#17, Distance#18, TaxiOut#20, Cancelled#21, CancellationCode#22, WeatherDelay#25, NASDelay#26, SecurityDelay#27, LateAircraftDelay#28]\n                                                                     +- Project [Year#0, Month#1, DayofMonth#2, DayOfWeek#3, DepTime#4, CRSDepTime#5, CRSArrTime#7, UniqueCarrier#8, FlightNum#9, TailNum#10, CRSElapsedTime#12, ArrDelay#14, DepDelay#15, Origin#16, Dest#17, Distance#18, TaxiOut#20, Cancelled#21, CancellationCode#22, CarrierDelay#24, WeatherDelay#25, NASDelay#26, SecurityDelay#27, LateAircraftDelay#28]\n                                                                        +- Project [Year#0, Month#1, DayofMonth#2, DayOfWeek#3, DepTime#4, CRSDepTime#5, CRSArrTime#7, UniqueCarrier#8, FlightNum#9, TailNum#10, CRSElapsedTime#12, ArrDelay#14, DepDelay#15, Origin#16, Dest#17, Distance#18, TaxiOut#20, Cancelled#21, CancellationCode#22, Diverted#23, CarrierDelay#24, WeatherDelay#25, NASDelay#26, SecurityDelay#27, LateAircraftDelay#28]\n                                                                           +- Project [Year#0, Month#1, DayofMonth#2, DayOfWeek#3, DepTime#4, CRSDepTime#5, CRSArrTime#7, UniqueCarrier#8, FlightNum#9, TailNum#10, CRSElapsedTime#12, ArrDelay#14, DepDelay#15, Origin#16, Dest#17, Distance#18, TaxiIn#19, TaxiOut#20, Cancelled#21, CancellationCode#22, Diverted#23, CarrierDelay#24, WeatherDelay#25, NASDelay#26, ... 2 more fields]\n                                                                              +- Project [Year#0, Month#1, DayofMonth#2, DayOfWeek#3, DepTime#4, CRSDepTime#5, CRSArrTime#7, UniqueCarrier#8, FlightNum#9, TailNum#10, CRSElapsedTime#12, AirTime#13, ArrDelay#14, DepDelay#15, Origin#16, Dest#17, Distance#18, TaxiIn#19, TaxiOut#20, Cancelled#21, CancellationCode#22, Diverted#23, CarrierDelay#24, WeatherDelay#25, ... 3 more fields]\n                                                                                 +- Project [Year#0, Month#1, DayofMonth#2, DayOfWeek#3, DepTime#4, CRSDepTime#5, CRSArrTime#7, UniqueCarrier#8, FlightNum#9, TailNum#10, ActualElapsedTime#11, CRSElapsedTime#12, AirTime#13, ArrDelay#14, DepDelay#15, Origin#16, Dest#17, Distance#18, TaxiIn#19, TaxiOut#20, Cancelled#21, CancellationCode#22, Diverted#23, CarrierDelay#24, ... 4 more fields]\n                                                                                    +- Relation [Year#0,Month#1,DayofMonth#2,DayOfWeek#3,DepTime#4,CRSDepTime#5,ArrTime#6,CRSArrTime#7,UniqueCarrier#8,FlightNum#9,TailNum#10,ActualElapsedTime#11,CRSElapsedTime#12,AirTime#13,ArrDelay#14,DepDelay#15,Origin#16,Dest#17,Distance#18,TaxiIn#19,TaxiOut#20,Cancelled#21,CancellationCode#22,Diverted#23,... 5 more fields] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11788/1371477804.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#    .otherwise((col(\"CRSDepTimeNewHour\").cast('long')*60 + col(\"CRSDepTimeNewMinute\")) - (col(\"DepTimeNewHour\").cast('long')*60 + col(\"DepTimeNewMinute\"))))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m df = df.withColumn(\"Duration\", \n\u001b[0m\u001b[0;32m      6\u001b[0m     (col(\"CRSArrTimeNewHour\").cast('long')*60 + col(\"CRSArrTimeNewMinute\").cast('long')) - (col(\"CRSDepTimeNewHour\").cast('long')*60 + col(\"CRSDepTimeNewMinute\").cast('long')))\n",
      "\u001b[1;32mc:\\users\\ofeucor\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mwithColumn\u001b[1;34m(self, colName, col)\u001b[0m\n\u001b[0;32m   2476\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2477\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"col should be Column\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2478\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolName\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2479\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2480\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwithColumnRenamed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexisting\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ofeucor\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1308\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1309\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1310\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1311\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ofeucor\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    115\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: Column 'CRSArrTimeNewHour' does not exist. Did you mean one of the following? [ArrDelay, CRSElapsedTime, Duration, Season, TailNum, DayOfWeek, FlightNum, Origin, TaxiOut, UniqueCarrier, Year, DepDelay, Dest, Distance, DayofMonth, Month];\n'Project [Year#0, Month#1, DayofMonth#2, DayOfWeek#3, UniqueCarrier#8, FlightNum#9, TailNum#10, CRSElapsedTime#12, ArrDelay#14, DepDelay#15, Origin#16, Dest#17, Distance#18, TaxiOut#20, Season#767, (((cast('CRSArrTimeNewHour as bigint) * 60) + cast('CRSArrTimeNewMinute as bigint)) - ((cast('CRSDepTimeNewHour as bigint) * 60) + cast('CRSDepTimeNewMinute as bigint))) AS Duration#2219]\n+- Project [Year#0, Month#1, DayofMonth#2, DayOfWeek#3, UniqueCarrier#8, FlightNum#9, TailNum#10, CRSElapsedTime#12, ArrDelay#14, DepDelay#15, Origin#16, Dest#17, Distance#18, TaxiOut#20, Season#767, Duration#1218]\n   +- Project [Year#0, Month#1, DayofMonth#2, DayOfWeek#3, DepTime#4, CRSDepTime#5, CRSArrTime#7, UniqueCarrier#8, FlightNum#9, TailNum#10, CRSElapsedTime#12, ArrDelay#14, DepDelay#15, Origin#16, Dest#17, Distance#18, TaxiOut#20, Season#767, DepTimeNew#838, CRSDepTimeNew#858, CRSArrTimeNew#879, CRSDepTimeNewHour#986, CRSDepTimeNewMinute#1009, DepTimeNewHour#1033, ... 4 more fields]\n      +- Project [Year#0, Month#1, DayofMonth#2, DayOfWeek#3, DepTime#4, CRSDepTime#5, CRSArrTime#7, UniqueCarrier#8, FlightNum#9, TailNum#10, CRSElapsedTime#12, ArrDelay#14, DepDelay#15, Origin#16, Dest#17, Distance#18, TaxiOut#20, Season#767, DepTimeNew#838, CRSDepTimeNew#858, CRSArrTimeNew#879, CRSDepTimeNewHour#986, CRSDepTimeNewMinute#1009, DepTimeNewHour#1033, ... 4 more fields]\n         +- Project [Year#0, Month#1, DayofMonth#2, DayOfWeek#3, DepTime#4, CRSDepTime#5, CRSArrTime#7, UniqueCarrier#8, FlightNum#9, TailNum#10, CRSElapsedTime#12, ArrDelay#14, DepDelay#15, Origin#16, Dest#17, Distance#18, TaxiOut#20, Season#767, DepTimeNew#838, CRSDepTimeNew#858, CRSArrTimeNew#879, CRSDepTimeNewHour#986, CRSDepTimeNewMinute#1009, DepTimeNewHour#1033, ... 3 more fields]\n            +- Project [Year#0, Month#1, DayofMonth#2, DayOfWeek#3, DepTime#4, CRSDepTime#5, CRSArrTime#7, UniqueCarrier#8, FlightNum#9, TailNum#10, CRSElapsedTime#12, ArrDelay#14, DepDelay#15, Origin#16, Dest#17, Distance#18, TaxiOut#20, Season#767, DepTimeNew#838, CRSDepTimeNew#858, CRSArrTimeNew#879, CRSDepTimeNewHour#986, CRSDepTimeNewMinute#1009, DepTimeNewHour#1033, ... 2 more fields]\n               +- Project [Year#0, Month#1, DayofMonth#2, DayOfWeek#3, DepTime#4, CRSDepTime#5, CRSArrTime#7, UniqueCarrier#8, FlightNum#9, TailNum#10, CRSElapsedTime#12, ArrDelay#14, DepDelay#15, Origin#16, Dest#17, Distance#18, TaxiOut#20, Season#767, DepTimeNew#838, CRSDepTimeNew#858, CRSArrTimeNew#879, CRSDepTimeNewHour#986, CRSDepTimeNewMinute#1009, DepTimeNewHour#1033, substring(DepTimeNew#838, 3, 2) AS DepTimeNewMinute#1058]\n                  +- Project [Year#0, Month#1, DayofMonth#2, DayOfWeek#3, DepTime#4, CRSDepTime#5, CRSArrTime#7, UniqueCarrier#8, FlightNum#9, TailNum#10, CRSElapsedTime#12, ArrDelay#14, DepDelay#15, Origin#16, Dest#17, Distance#18, TaxiOut#20, Season#767, DepTimeNew#838, CRSDepTimeNew#858, CRSArrTimeNew#879, CRSDepTimeNewHour#986, CRSDepTimeNewMinute#1009, substring(DepTimeNew#838, 1, 2) AS DepTimeNewHour#1033]\n                     +- Project [Year#0, Month#1, DayofMonth#2, DayOfWeek#3, DepTime#4, CRSDepTime#5, CRSArrTime#7, UniqueCarrier#8, FlightNum#9, TailNum#10, CRSElapsedTime#12, ArrDelay#14, DepDelay#15, Origin#16, Dest#17, Distance#18, TaxiOut#20, Season#767, DepTimeNew#838, CRSDepTimeNew#858, CRSArrTimeNew#879, CRSDepTimeNewHour#986, substring(CRSDepTimeNew#858, 3, 2) AS CRSDepTimeNewMinute#1009]\n                        +- Project [Year#0, Month#1, DayofMonth#2, DayOfWeek#3, DepTime#4, CRSDepTime#5, CRSArrTime#7, UniqueCarrier#8, FlightNum#9, TailNum#10, CRSElapsedTime#12, ArrDelay#14, DepDelay#15, Origin#16, Dest#17, Distance#18, TaxiOut#20, Season#767, DepTimeNew#838, CRSDepTimeNew#858, CRSArrTimeNew#879, substring(CRSDepTimeNew#858, 1, 2) AS CRSDepTimeNewHour#986]\n                           +- Project [Year#0, Month#1, DayofMonth#2, DayOfWeek#3, DepTime#4, CRSDepTime#5, CRSArrTime#7, UniqueCarrier#8, FlightNum#9, TailNum#10, CRSElapsedTime#12, ArrDelay#14, DepDelay#15, Origin#16, Dest#17, Distance#18, TaxiOut#20, Season#767, DepTimeNew#838, CRSDepTimeNew#858, CASE WHEN (length(cast(CRSArrTime#7 as string)) = 3) THEN concat(0, cast(CRSArrTime#7 as string)) WHEN (length(cast(CRSArrTime#7 as string)) = 2) THEN concat(00, cast(CRSArrTime#7 as string)) ELSE cast(CRSArrTime#7 as string) END AS CRSArrTimeNew#879]\n                              +- Project [Year#0, Month#1, DayofMonth#2, DayOfWeek#3, DepTime#4, CRSDepTime#5, CRSArrTime#7, UniqueCarrier#8, FlightNum#9, TailNum#10, CRSElapsedTime#12, ArrDelay#14, DepDelay#15, Origin#16, Dest#17, Distance#18, TaxiOut#20, Season#767, DepTimeNew#838, CASE WHEN (length(cast(CRSDepTime#5 as string)) = 3) THEN concat(0, cast(CRSDepTime#5 as string)) WHEN (length(cast(CRSDepTime#5 as string)) = 2) THEN concat(00, cast(CRSDepTime#5 as string)) ELSE cast(CRSDepTime#5 as string) END AS CRSDepTimeNew#858]\n                                 +- Project [Year#0, Month#1, DayofMonth#2, DayOfWeek#3, DepTime#4, CRSDepTime#5, CRSArrTime#7, UniqueCarrier#8, FlightNum#9, TailNum#10, CRSElapsedTime#12, ArrDelay#14, DepDelay#15, Origin#16, Dest#17, Distance#18, TaxiOut#20, Season#767, CASE WHEN (length(cast(DepTime#4 as string)) = 3) THEN concat(0, cast(DepTime#4 as string)) WHEN (length(cast(DepTime#4 as string)) = 2) THEN concat(00, cast(DepTime#4 as string)) ELSE cast(DepTime#4 as string) END AS DepTimeNew#838]\n                                    +- Project [Year#0, Month#1, DayofMonth#2, DayOfWeek#3, DepTime#4, CRSDepTime#5, CRSArrTime#7, UniqueCarrier#8, FlightNum#9, TailNum#10, CRSElapsedTime#12, ArrDelay#14, DepDelay#15, Origin#16, Dest#17, Distance#18, TaxiOut#20, CASE WHEN ((Month#1 > 2) AND (Month#1 < 6)) THEN 1 WHEN ((Month#1 > 5) AND (Month#1 < 9)) THEN 2 WHEN ((Month#1 > 8) AND (Month#1 < 12)) THEN 3 ELSE 4 END AS Season#767]\n                                       +- Filter atleastnnonnulls(17, Year#0, Month#1, DayofMonth#2, DayOfWeek#3, DepTime#4, CRSDepTime#5, CRSArrTime#7, UniqueCarrier#8, FlightNum#9, TailNum#10, CRSElapsedTime#12, ArrDelay#14, DepDelay#15, Origin#16, Dest#17, Distance#18, TaxiOut#20)\n                                          +- Project [Year#0, Month#1, DayofMonth#2, DayOfWeek#3, DepTime#4, CRSDepTime#5, CRSArrTime#7, UniqueCarrier#8, FlightNum#9, TailNum#10, CRSElapsedTime#12, ArrDelay#14, DepDelay#15, Origin#16, Dest#17, Distance#18, TaxiOut#20]\n                                             +- Filter isnull(CancellationCode#22)\n                                                +- Filter (Cancelled#21 = 0)\n                                                   +- Deduplicate [TaxiOut#20, CRSDepTime#5, Month#1, Dest#17, Origin#16, DepTime#4, DayOfWeek#3, Year#0, UniqueCarrier#8, ArrDelay#14, Distance#18, DepDelay#15, FlightNum#9, CRSElapsedTime#12, CancellationCode#22, DayofMonth#2, CRSArrTime#7, TailNum#10, Cancelled#21]\n                                                      +- Project [Year#0, Month#1, DayofMonth#2, DayOfWeek#3, DepTime#4, CRSDepTime#5, CRSArrTime#7, UniqueCarrier#8, FlightNum#9, TailNum#10, CRSElapsedTime#12, ArrDelay#14, DepDelay#15, Origin#16, Dest#17, Distance#18, TaxiOut#20, Cancelled#21, CancellationCode#22]\n                                                         +- Project [Year#0, Month#1, DayofMonth#2, DayOfWeek#3, DepTime#4, CRSDepTime#5, CRSArrTime#7, UniqueCarrier#8, FlightNum#9, TailNum#10, CRSElapsedTime#12, ArrDelay#14, DepDelay#15, Origin#16, Dest#17, Distance#18, TaxiOut#20, Cancelled#21, CancellationCode#22, LateAircraftDelay#28]\n                                                            +- Project [Year#0, Month#1, DayofMonth#2, DayOfWeek#3, DepTime#4, CRSDepTime#5, CRSArrTime#7, UniqueCarrier#8, FlightNum#9, TailNum#10, CRSElapsedTime#12, ArrDelay#14, DepDelay#15, Origin#16, Dest#17, Distance#18, TaxiOut#20, Cancelled#21, CancellationCode#22, SecurityDelay#27, LateAircraftDelay#28]\n                                                               +- Project [Year#0, Month#1, DayofMonth#2, DayOfWeek#3, DepTime#4, CRSDepTime#5, CRSArrTime#7, UniqueCarrier#8, FlightNum#9, TailNum#10, CRSElapsedTime#12, ArrDelay#14, DepDelay#15, Origin#16, Dest#17, Distance#18, TaxiOut#20, Cancelled#21, CancellationCode#22, NASDelay#26, SecurityDelay#27, LateAircraftDelay#28]\n                                                                  +- Project [Year#0, Month#1, DayofMonth#2, DayOfWeek#3, DepTime#4, CRSDepTime#5, CRSArrTime#7, UniqueCarrier#8, FlightNum#9, TailNum#10, CRSElapsedTime#12, ArrDelay#14, DepDelay#15, Origin#16, Dest#17, Distance#18, TaxiOut#20, Cancelled#21, CancellationCode#22, WeatherDelay#25, NASDelay#26, SecurityDelay#27, LateAircraftDelay#28]\n                                                                     +- Project [Year#0, Month#1, DayofMonth#2, DayOfWeek#3, DepTime#4, CRSDepTime#5, CRSArrTime#7, UniqueCarrier#8, FlightNum#9, TailNum#10, CRSElapsedTime#12, ArrDelay#14, DepDelay#15, Origin#16, Dest#17, Distance#18, TaxiOut#20, Cancelled#21, CancellationCode#22, CarrierDelay#24, WeatherDelay#25, NASDelay#26, SecurityDelay#27, LateAircraftDelay#28]\n                                                                        +- Project [Year#0, Month#1, DayofMonth#2, DayOfWeek#3, DepTime#4, CRSDepTime#5, CRSArrTime#7, UniqueCarrier#8, FlightNum#9, TailNum#10, CRSElapsedTime#12, ArrDelay#14, DepDelay#15, Origin#16, Dest#17, Distance#18, TaxiOut#20, Cancelled#21, CancellationCode#22, Diverted#23, CarrierDelay#24, WeatherDelay#25, NASDelay#26, SecurityDelay#27, LateAircraftDelay#28]\n                                                                           +- Project [Year#0, Month#1, DayofMonth#2, DayOfWeek#3, DepTime#4, CRSDepTime#5, CRSArrTime#7, UniqueCarrier#8, FlightNum#9, TailNum#10, CRSElapsedTime#12, ArrDelay#14, DepDelay#15, Origin#16, Dest#17, Distance#18, TaxiIn#19, TaxiOut#20, Cancelled#21, CancellationCode#22, Diverted#23, CarrierDelay#24, WeatherDelay#25, NASDelay#26, ... 2 more fields]\n                                                                              +- Project [Year#0, Month#1, DayofMonth#2, DayOfWeek#3, DepTime#4, CRSDepTime#5, CRSArrTime#7, UniqueCarrier#8, FlightNum#9, TailNum#10, CRSElapsedTime#12, AirTime#13, ArrDelay#14, DepDelay#15, Origin#16, Dest#17, Distance#18, TaxiIn#19, TaxiOut#20, Cancelled#21, CancellationCode#22, Diverted#23, CarrierDelay#24, WeatherDelay#25, ... 3 more fields]\n                                                                                 +- Project [Year#0, Month#1, DayofMonth#2, DayOfWeek#3, DepTime#4, CRSDepTime#5, CRSArrTime#7, UniqueCarrier#8, FlightNum#9, TailNum#10, ActualElapsedTime#11, CRSElapsedTime#12, AirTime#13, ArrDelay#14, DepDelay#15, Origin#16, Dest#17, Distance#18, TaxiIn#19, TaxiOut#20, Cancelled#21, CancellationCode#22, Diverted#23, CarrierDelay#24, ... 4 more fields]\n                                                                                    +- Relation [Year#0,Month#1,DayofMonth#2,DayOfWeek#3,DepTime#4,CRSDepTime#5,ArrTime#6,CRSArrTime#7,UniqueCarrier#8,FlightNum#9,TailNum#10,ActualElapsedTime#11,CRSElapsedTime#12,AirTime#13,ArrDelay#14,DepDelay#15,Origin#16,Dest#17,Distance#18,TaxiIn#19,TaxiOut#20,Cancelled#21,CancellationCode#22,Diverted#23,... 5 more fields] csv\n"
     ]
    }
   ],
   "source": [
    "#df = df.withColumn(\"DepDelayNew\", when(abs((col(\"DepTimeNewHour\").cast('long')*60 + col(\"DepTimeNewMinute\")) - (col(\"CRSDepTimeNewHour\").cast('long')*60 + col(\"CRSDepTimeNewMinute\"))) < abs((col(\"CRSDepTimeNewHour\").cast('long')*60 + col(\"CRSDepTimeNewMinute\")) - (col(\"DepTimeNewHour\").cast('long')*60 + col(\"DepTimeNewMinute\"))), \n",
    "#    (col(\"DepTimeNewHour\").cast('long')*60 + col(\"DepTimeNewMinute\")) - (col(\"CRSDepTimeNewHour\").cast('long')*60 + col(\"CRSDepTimeNewMinute\"))) \\\n",
    "#    .otherwise((col(\"CRSDepTimeNewHour\").cast('long')*60 + col(\"CRSDepTimeNewMinute\")) - (col(\"DepTimeNewHour\").cast('long')*60 + col(\"DepTimeNewMinute\"))))\n",
    "                   \n",
    "df = df.withColumn(\"Duration\", \n",
    "    (col(\"CRSArrTimeNewHour\").cast('long')*60 + col(\"CRSArrTimeNewMinute\").cast('long')) - (col(\"CRSDepTimeNewHour\").cast('long')*60 + col(\"CRSDepTimeNewMinute\").cast('long')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dressed-distributor",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.withColumn(\"DepDelayNew\",col(\"DepDelayNew\").cast(IntegerType()))\n",
    "df = df.withColumn(\"Duration\",col(\"Duration\").cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "satisfactory-furniture",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.select(\"DepTimeNew\",\"CRSDepTimeNew\",\"DepDelay\",\"DepDelayNew\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "elementary-spare",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+--------+\n",
      "|CRSArrTimeNew|CRSDepTimeNew|Duration|\n",
      "+-------------+-------------+--------+\n",
      "|         1324|         1220|      64|\n",
      "|         1711|         1409|     182|\n",
      "|         1845|         1755|      50|\n",
      "|         0830|         0655|      95|\n",
      "|         0825|         0705|      80|\n",
      "|         0859|         0737|      82|\n",
      "|         0835|         0740|      55|\n",
      "|         0845|         0745|      60|\n",
      "|         1105|         0850|     135|\n",
      "|         1015|         0908|      67|\n",
      "|         1030|         0935|      55|\n",
      "|         1105|         0940|      85|\n",
      "|         1115|         1010|      65|\n",
      "|         1140|         1040|      60|\n",
      "|         1140|         1040|      60|\n",
      "|         1245|         1136|      69|\n",
      "|         1415|         1300|      75|\n",
      "|         1512|         1315|     117|\n",
      "|         1650|         1425|     145|\n",
      "|         1516|         1436|      40|\n",
      "+-------------+-------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"CRSArrTimeNew\",\"CRSDepTimeNew\",\"Duration\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "arranged-investing",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(\"CRSDepTime\", \"CRSArrTime\", \"DepTime\", 'DepTimeNew', 'CRSDepTimeNew', 'CRSArrTimeNew', 'CRSDepTimeNewHour',\n",
    " 'CRSDepTimeNewMinute', 'DepTimeNewHour',  'DepTimeNewMinute', 'DepDelayNew', 'CRSArrTimeNewHour', 'CRSArrTimeNewMinute')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "express-working",
   "metadata": {},
   "source": [
    "## Concordancy between related variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "established-battlefield",
   "metadata": {},
   "source": [
    "**No flights with same Origin and Destination**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "driving-length",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+---------+-------------+---------+-------+--------------+--------+--------+------+----+--------+-------+------+--------+\n",
      "|Year|Month|DayofMonth|DayOfWeek|UniqueCarrier|FlightNum|TailNum|CRSElapsedTime|ArrDelay|DepDelay|Origin|Dest|Distance|TaxiOut|Season|Duration|\n",
      "+----+-----+----------+---------+-------------+---------+-------+--------------+--------+--------+------+----+--------+-------+------+--------+\n",
      "+----+-----+----------+---------+-------------+---------+-------+--------------+--------+--------+------+----+--------+-------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# quitarlos directamente para automatizar el proceso?\n",
    "df.filter(df.Origin == df.Dest).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "material-handbook",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- DayofMonth: integer (nullable = true)\n",
      " |-- DayOfWeek: integer (nullable = true)\n",
      " |-- UniqueCarrier: string (nullable = true)\n",
      " |-- FlightNum: integer (nullable = true)\n",
      " |-- TailNum: string (nullable = true)\n",
      " |-- CRSElapsedTime: integer (nullable = true)\n",
      " |-- ArrDelay: integer (nullable = true)\n",
      " |-- DepDelay: integer (nullable = true)\n",
      " |-- Origin: string (nullable = true)\n",
      " |-- Dest: string (nullable = true)\n",
      " |-- Distance: integer (nullable = true)\n",
      " |-- TaxiOut: integer (nullable = true)\n",
      " |-- Season: integer (nullable = false)\n",
      " |-- Duration: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emotional-funeral",
   "metadata": {},
   "source": [
    "## Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "respective-softball",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name              Count %\n",
      "--------------  ---------\n",
      "Year                    0\n",
      "Month                   0\n",
      "DayofMonth              0\n",
      "DayOfWeek               0\n",
      "UniqueCarrier           0\n",
      "FlightNum               0\n",
      "TailNum                 0\n",
      "CRSElapsedTime          0\n",
      "ArrDelay                0\n",
      "DepDelay                0\n",
      "Origin                  0\n",
      "Dest                    0\n",
      "Distance                0\n",
      "TaxiOut                 0\n",
      "Season                  0\n",
      "Duration             3879\n"
     ]
    }
   ],
   "source": [
    "print(tabulate([[c, df.filter(col(c).isNull()).count()] for c in df.columns], headers=['Name', 'Count %']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "minus-association",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+---------+-------------+---------+-------+--------------+--------+--------+------+----+--------+-------+------+--------+\n",
      "|Year|Month|DayofMonth|DayOfWeek|UniqueCarrier|FlightNum|TailNum|CRSElapsedTime|ArrDelay|DepDelay|Origin|Dest|Distance|TaxiOut|Season|Duration|\n",
      "+----+-----+----------+---------+-------------+---------+-------+--------------+--------+--------+------+----+--------+-------+------+--------+\n",
      "|2008|    1|        25|        5|           DL|      690| N619DL|           151|     -24|      -3|   ATL| BOS|     946|     11|     4|    null|\n",
      "|2008|    1|         5|        6|           WN|     2025| N776WN|           200|      49|      57|   LAS| MDW|    1521|     10|     4|    null|\n",
      "|2008|    1|        18|        5|           WN|     1747| N451WN|           190|      11|      28|   PHX| BNA|    1448|      7|     4|    null|\n",
      "|2008|    1|        25|        5|           OH|     4948| N597SW|           139|      11|      26|   LGA| TYS|     647|     15|     4|    null|\n",
      "|2008|    1|        28|        1|           UA|      430| N831UA|           213|     -12|       3|   DEN| EWR|    1605|     14|     4|    null|\n",
      "|2008|    1|         1|        2|           WN|     2786| N424WN|           100|     125|     137|   MDW| BWI|     611|      8|     4|    null|\n",
      "|2008|    1|        16|        3|           WN|     2295| N507SW|           120|     -10|      -7|   PHX| SAT|     843|     14|     4|    null|\n",
      "|2008|    1|        22|        2|           WN|     1747| N741SA|           190|      27|      50|   PHX| BNA|    1448|      8|     4|    null|\n",
      "|2008|    1|        14|        1|           YV|     2611| N906FJ|           173|       7|       0|   CLT| IAH|     913|     21|     4|    null|\n",
      "|2008|    1|        20|        7|           US|      296| N678AW|           157|      -1|       0|   LAS| SEA|     866|     19|     4|    null|\n",
      "|2008|    1|        19|        6|           WN|      151| N482WN|           180|      56|      70|   LAS| STL|    1372|      9|     4|    null|\n",
      "|2008|    1|         6|        7|           OO|     6470| N719SK|           105|      62|      55|   LAX| SLC|     590|     32|     4|    null|\n",
      "|2008|    1|        28|        1|           UA|     1190| N938UA|           166|     -19|      -1|   DEN| DTW|    1123|     10|     4|    null|\n",
      "|2008|    1|         7|        1|           US|      959| N202UW|           118|      -7|      10|   CLT| FLL|     631|      8|     4|    null|\n",
      "|2008|    1|        12|        6|           US|      959| N201UU|           118|     -22|     -10|   CLT| FLL|     631|     11|     4|    null|\n",
      "|2008|    1|         4|        5|           WN|      632| N268WN|           110|      21|      30|   LAS| DEN|     629|     16|     4|    null|\n",
      "|2008|    1|        17|        4|           YV|     2611| N903FJ|           173|       0|      12|   CLT| IAH|     913|     15|     4|    null|\n",
      "|2008|    1|        17|        4|           UA|     1190| N912UA|           166|     -13|      -4|   DEN| DTW|    1123|     12|     4|    null|\n",
      "|2008|    1|        14|        1|           WN|      904| N650SW|           105|       1|      -1|   LAX| SLC|     590|     12|     4|    null|\n",
      "|2008|    1|         7|        1|           OH|     4948| N435CA|           139|     -13|       0|   LGA| TYS|     647|     18|     4|    null|\n",
      "+----+-----+----------+---------+-------------+---------+-------+--------------+--------+--------+------+----+--------+-------+------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.Duration.isNull()).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "immune-reply",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "imputer = Imputer(\n",
    "    inputCols=['Duration'], \n",
    "    outputCols=['Duration']\n",
    ")\n",
    "\n",
    "df = imputer.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "juvenile-layout",
   "metadata": {},
   "source": [
    "## Fix format of variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "engaging-vietnam",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop( 'Date',  'datetime', 'FlightNum', 'TailNum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "integral-classroom",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Year',\n",
       " 'Month',\n",
       " 'DayofMonth',\n",
       " 'DayOfWeek',\n",
       " 'UniqueCarrier',\n",
       " 'CRSElapsedTime',\n",
       " 'ArrDelay',\n",
       " 'DepDelay',\n",
       " 'Origin',\n",
       " 'Dest',\n",
       " 'Distance',\n",
       " 'TaxiOut',\n",
       " 'Season',\n",
       " 'Duration']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "hired-bowling",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+---------+-------------+--------------+--------+--------+------+----+--------+-------+------+--------+\n",
      "|Year|Month|DayofMonth|DayOfWeek|UniqueCarrier|CRSElapsedTime|ArrDelay|DepDelay|Origin|Dest|Distance|TaxiOut|Season|Duration|\n",
      "+----+-----+----------+---------+-------------+--------------+--------+--------+------+----+--------+-------+------+--------+\n",
      "|2008|    1|        28|        1|           EV|            64|     -19|      -5|   AGS| ATL|     143|      3|     4|      64|\n",
      "|2008|    1|        27|        7|           YV|           122|      69|      91|   ORD| SAV|     773|      3|     4|     182|\n",
      "|2008|    1|         4|        5|           OO|            50|      42|      40|   PDX| SEA|     129|      4|     4|      50|\n",
      "|2008|    1|        21|        1|           WN|            95|      -4|      14|   DAL| BHM|     587|      5|     4|      95|\n",
      "|2008|    1|        21|        1|           WN|            80|     -11|      -5|   MSY| BNA|     471|      5|     4|      80|\n",
      "+----+-----+----------+---------+-------------+--------------+--------+--------+------+----+--------+-------+------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "interim-mechanism",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|            features|ArrDelay|\n",
      "+--------------------+--------+\n",
      "|[2008.0,1.0,28.0,...|     -19|\n",
      "|[2008.0,1.0,27.0,...|      69|\n",
      "|[2008.0,1.0,4.0,5...|      42|\n",
      "+--------------------+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "training = df.drop( 'Date',  'datetime', 'FlightNum', 'TailNum', 'UniqueCarrier', 'Origin', 'Dest')\n",
    "training = training.select('Year', 'Month', 'DayofMonth', 'DayOfWeek', 'CRSElapsedTime',\n",
    " 'DepDelay', 'Distance', 'TaxiOut', 'Season', 'Duration', 'ArrDelay')\n",
    "\n",
    "vectorAssembler = VectorAssembler(inputCols = ['Year', 'Month', 'DayofMonth', 'DayOfWeek', 'CRSElapsedTime',\n",
    " 'DepDelay', 'Distance', 'TaxiOut', 'Season', 'Duration'], outputCol = 'features')\n",
    "vdf = vectorAssembler.transform(training)\n",
    "vdf = vdf.select(['features', 'ArrDelay'])\n",
    "vdf.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "monetary-conviction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+---------+--------------+--------+--------+-------+------+--------+--------+\n",
      "|Year|Month|DayofMonth|DayOfWeek|CRSElapsedTime|DepDelay|Distance|TaxiOut|Season|Duration|ArrDelay|\n",
      "+----+-----+----------+---------+--------------+--------+--------+-------+------+--------+--------+\n",
      "|2008|    1|        28|        1|            64|      -5|     143|      3|     4|      64|     -19|\n",
      "|2008|    1|        27|        7|           122|      91|     773|      3|     4|     182|      69|\n",
      "|2008|    1|         4|        5|            50|      40|     129|      4|     4|      50|      42|\n",
      "|2008|    1|        21|        1|            95|      14|     587|      5|     4|      95|      -4|\n",
      "|2008|    1|        21|        1|            80|      -5|     471|      5|     4|      80|     -11|\n",
      "+----+-----+----------+---------+--------------+--------+--------+-------+------+--------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "social-newport",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = vdf.randomSplit([0.7, 0.3])\n",
    "train_df = splits[0]\n",
    "test_df = splits[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "swedish-pacific",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression(featuresCol = 'features', labelCol='ArrDelay', maxIter=10, regParam=0.3, elasticNetParam=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "handed-reporter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [0.0,0.0,0.0,0.0,-0.027314036042924667,0.986464511194288,0.0,0.7983756045738953,0.0,0.0]\n",
      "Intercept: -10.73518439003692\n"
     ]
    }
   ],
   "source": [
    "lr_model = lr.fit(train_df)\n",
    "print(\"Coefficients: \" + str(lr_model.coefficients))\n",
    "print(\"Intercept: \" + str(lr_model.intercept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "considered-senator",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 10.973455\n",
      "r2: 0.923366\n"
     ]
    }
   ],
   "source": [
    "trainingSummary = lr_model.summary\n",
    "print(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)\n",
    "print(\"r2: %f\" % trainingSummary.r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "spare-astronomy",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 62.0 failed 1 times, most recent failure: Lost task 0.0 in stage 62.0 (TID 180) (DESKTOP-6TQUAIA executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(DualStackPlainSocketImpl.java:135)\r\n\tat java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409)\r\n\tat java.net.PlainSocketImpl.accept(PlainSocketImpl.java:199)\r\n\tat java.net.ServerSocket.implAccept(ServerSocket.java:560)\r\n\tat java.net.ServerSocket.accept(ServerSocket.java:528)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n\t... 14 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n\tat scala.collection.immutable.List.foreach(List.scala:333)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r\n\tat scala.Option.foreach(Option.scala:437)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\r\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:166)\r\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(DualStackPlainSocketImpl.java:135)\r\n\tat java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409)\r\n\tat java.net.PlainSocketImpl.accept(PlainSocketImpl.java:199)\r\n\tat java.net.ServerSocket.implAccept(ServerSocket.java:560)\r\n\tat java.net.ServerSocket.accept(ServerSocket.java:528)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n\t... 14 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20356/2268731779.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m training = training.select('Year', 'Month', 'DayofMonth', 'DayOfWeek', 'CRSElapsedTime',\n\u001b[0;32m      4\u001b[0m  'DepDelay', 'Distance', 'TaxiOut', 'Season', 'Duration', 'ArrDelay')\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mtrainingData\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mVectors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoDF\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"features\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"label\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ofeucor\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36mtoDF\u001b[1;34m(self, schema, sampleRatio)\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Alice'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m         \"\"\"\n\u001b[1;32m---> 66\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msparkSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msampleRatio\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[0mRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoDF\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtoDF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ofeucor\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m    673\u001b[0m             return super(SparkSession, self).createDataFrame(\n\u001b[0;32m    674\u001b[0m                 data, schema, samplingRatio, verifySchema)\n\u001b[1;32m--> 675\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_dataframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    676\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    677\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_create_dataframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ofeucor\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36m_create_dataframe\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m    696\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    697\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 698\u001b[1;33m             \u001b[0mrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_createFromRDD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    699\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    700\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ofeucor\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36m_createFromRDD\u001b[1;34m(self, rdd, schema, samplingRatio)\u001b[0m\n\u001b[0;32m    484\u001b[0m         \"\"\"\n\u001b[0;32m    485\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mschema\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 486\u001b[1;33m             \u001b[0mstruct\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inferSchema\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mschema\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    487\u001b[0m             \u001b[0mconverter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_create_converter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstruct\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m             \u001b[0mrdd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconverter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ofeucor\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36m_inferSchema\u001b[1;34m(self, rdd, samplingRatio, names)\u001b[0m\n\u001b[0;32m    458\u001b[0m         \u001b[1;33m:\u001b[0m\u001b[1;32mclass\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStructType\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m         \"\"\"\n\u001b[1;32m--> 460\u001b[1;33m         \u001b[0mfirst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    461\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mfirst\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    462\u001b[0m             raise ValueError(\"The first row in RDD is empty, \"\n",
      "\u001b[1;32mc:\\users\\ofeucor\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mfirst\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1586\u001b[0m         \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mRDD\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mempty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1587\u001b[0m         \"\"\"\n\u001b[1;32m-> 1588\u001b[1;33m         \u001b[0mrs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1589\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1590\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mrs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ofeucor\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mtake\u001b[1;34m(self, num)\u001b[0m\n\u001b[0;32m   1566\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1567\u001b[0m             \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1568\u001b[1;33m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1570\u001b[0m             \u001b[0mitems\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ofeucor\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36mrunJob\u001b[1;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[0;32m   1225\u001b[0m         \u001b[1;31m# SparkContext#runJob.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1226\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1227\u001b[1;33m         \u001b[0msock_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1228\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1229\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ofeucor\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1308\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1309\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1310\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1311\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ofeucor\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ofeucor\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 62.0 failed 1 times, most recent failure: Lost task 0.0 in stage 62.0 (TID 180) (DESKTOP-6TQUAIA executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(DualStackPlainSocketImpl.java:135)\r\n\tat java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409)\r\n\tat java.net.PlainSocketImpl.accept(PlainSocketImpl.java:199)\r\n\tat java.net.ServerSocket.implAccept(ServerSocket.java:560)\r\n\tat java.net.ServerSocket.accept(ServerSocket.java:528)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n\t... 14 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n\tat scala.collection.immutable.List.foreach(List.scala:333)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r\n\tat scala.Option.foreach(Option.scala:437)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\r\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:166)\r\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(DualStackPlainSocketImpl.java:135)\r\n\tat java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409)\r\n\tat java.net.PlainSocketImpl.accept(PlainSocketImpl.java:199)\r\n\tat java.net.ServerSocket.implAccept(ServerSocket.java:560)\r\n\tat java.net.ServerSocket.accept(ServerSocket.java:528)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n\t... 14 more\r\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "training = df.drop( 'Date',  'datetime', 'FlightNum', 'TailNum', 'UniqueCarrier', 'Origin', 'Dest')\n",
    "training = training.select('Year', 'Month', 'DayofMonth', 'DayOfWeek', 'CRSElapsedTime',\n",
    " 'DepDelay', 'Distance', 'TaxiOut', 'Season', 'Duration', 'ArrDelay')\n",
    "trainingData=training.rdd.map(lambda x:(Vectors.dense(x[0:-1]), x[-1])).toDF([\"features\", \"label\"])\n",
    "\n",
    "\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "\n",
    "# Fit the model\n",
    "lrModel = lr.fit(trainingData)\n",
    "\n",
    "# Print the coefficients and intercept for logistic regression\n",
    "print(\"Coefficients: \" + str(lrModel.coefficients))\n",
    "print(\"Intercept: \" + str(lrModel.intercept))\n",
    "\n",
    "# We can also use the multinomial family for binary classification\n",
    "mlr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8, family=\"multinomial\")\n",
    "\n",
    "# Fit the model\n",
    "mlrModel = mlr.fit(training)\n",
    "\n",
    "# Print the coefficients and intercepts for logistic regression with multinomial family\n",
    "print(\"Multinomial coefficients: \" + str(mlrModel.coefficientMatrix))\n",
    "print(\"Multinomial intercepts: \" + str(mlrModel.interceptVector))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civilian-shower",
   "metadata": {},
   "source": [
    "# Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "collaborative-monroe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o274.randomSplit.\n: java.lang.ClassCastException: java.lang.Integer cannot be cast to java.lang.Double\r\n\tat scala.runtime.BoxesRunTime.unboxToDouble(BoxesRunTime.java:112)\r\n\tat scala.collection.mutable.ArrayBuilder$ofDouble.addOne(ArrayBuilder.scala:402)\r\n\tat scala.collection.mutable.Growable.addAll(Growable.scala:62)\r\n\tat scala.collection.mutable.Growable.addAll$(Growable.scala:57)\r\n\tat scala.collection.mutable.ArrayBuilder.addAll(ArrayBuilder.scala:66)\r\n\tat scala.collection.IterableOnceOps.toArray(IterableOnce.scala:1282)\r\n\tat scala.collection.IterableOnceOps.toArray$(IterableOnce.scala:1276)\r\n\tat scala.collection.AbstractIterable.toArray(Iterable.scala:926)\r\n\tat org.apache.spark.sql.Dataset.randomSplit(Dataset.scala:2378)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20356/3829561142.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# split into training(60%), validation(20%) and test(20%) datasets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrainingDf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidationDf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestDf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandomSplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#print(trainingDf.take(1))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ofeucor\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mrandomSplit\u001b[1;34m(self, weights, seed)\u001b[0m\n\u001b[0;32m   1186\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Weights must be positive. Found weight value: %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1187\u001b[0m         \u001b[0mseed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mseed\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mseed\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaxsize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1188\u001b[1;33m         \u001b[0mrdd_array\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandomSplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_to_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1189\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mrdd\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrdd_array\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ofeucor\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1308\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1309\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1310\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1311\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ofeucor\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ofeucor\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o274.randomSplit.\n: java.lang.ClassCastException: java.lang.Integer cannot be cast to java.lang.Double\r\n\tat scala.runtime.BoxesRunTime.unboxToDouble(BoxesRunTime.java:112)\r\n\tat scala.collection.mutable.ArrayBuilder$ofDouble.addOne(ArrayBuilder.scala:402)\r\n\tat scala.collection.mutable.Growable.addAll(Growable.scala:62)\r\n\tat scala.collection.mutable.Growable.addAll$(Growable.scala:57)\r\n\tat scala.collection.mutable.ArrayBuilder.addAll(ArrayBuilder.scala:66)\r\n\tat scala.collection.IterableOnceOps.toArray(IterableOnce.scala:1282)\r\n\tat scala.collection.IterableOnceOps.toArray$(IterableOnce.scala:1276)\r\n\tat scala.collection.AbstractIterable.toArray(Iterable.scala:926)\r\n\tat org.apache.spark.sql.Dataset.randomSplit(Dataset.scala:2378)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n"
     ]
    }
   ],
   "source": [
    "# split into training(60%), validation(20%) and test(20%) datasets\n",
    "trainingDf, validationDf, testDf = df.randomSplit([7, 1, 2])\n",
    "\n",
    "#print(trainingDf.take(1))\n",
    "\n",
    "#lets cache these datasets\n",
    "trainingDf.cache()\n",
    "validationDf.cache()\n",
    "testDf.cache()\n",
    "\n",
    "print(\"Num of training observations : %s\" % trainingDf.count())\n",
    "print(\"Num of validation observations : %s\" % validationRdd.count())\n",
    "print(\"Num of test observations : %s\" % testDf.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "institutional-questionnaire",
   "metadata": {},
   "source": [
    "## Categoricals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perfect-surge",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the categorical attributes to binary features\n",
    "categoricalAttributes = ['Origin', 'Dest', 'UniqueCarrier']\n",
    "\n",
    "#Build a list of pipelist stages for the machine learning pipeline. \n",
    "#start by the feature transformer of one hot encoder for building the categorical features\n",
    "pipelineStages = []\n",
    "for columnName in categoricalAttributes:\n",
    "    stringIndexer = StringIndexer(inputCol=columnName, outputCol=columnName+ \"Index\")\n",
    "    pipelineStages.append(stringIndexer)\n",
    "    oneHotEncoder = OneHotEncoder(inputCol=columnName+ \"Index\", outputCol=columnName + \"Vec\")\n",
    "    pipelineStages.append(oneHotEncoder)\n",
    "    \n",
    "    \n",
    "print(\"%s string indexer and one hot encoders transformers\" %  len(pipelineStages) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abandoned-resort",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all the feature columns into a single column in the dataframe\n",
    "numericColumns = ['Year', 'Month', 'DayofMonth', 'DayOfWeek',\n",
    " 'UniqueCarrier', \n",
    " 'CRSElapsedTime', 'ArrDelay', 'DepDelay',\n",
    " 'Origin', 'Dest', 'Distance', 'TaxiOut', 'Season', 'Duration']\n",
    "categoricalCols = [s + \"Vec\" for s in categoricalAttributes]\n",
    "allFeatureCols =  numericColumns + categoricalCols\n",
    "vectorAssembler = VectorAssembler(\n",
    "    inputCols=allFeatureCols,\n",
    "    outputCol=\"features\")\n",
    "pipelineStages.append(vectorAssembler)\n",
    "print(\"%s feature columns: %s\" % (len(allFeatureCols),allFeatureCols))\n",
    "\n",
    "\n",
    "#Build pipeline for feature extraction\n",
    "featurePipeline = Pipeline(stages=pipelineStages)\n",
    "featureOnlyModel = featurePipeline.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "objective-prayer",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create list of Dataframes with features\n",
    "trainingFeaturesDf = featureOnlyModel.transform(df)\n",
    "validationFeaturesDf = featureOnlyModel.transform(validationDf)\n",
    "testFeaturesDf = featureOnlyModel.transform(testDf)\n",
    "\n",
    "#peek\n",
    "trainingFeaturesDf.select(\"features\", \"label\").rdd.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "italic-winning",
   "metadata": {},
   "source": [
    "# Building A Machine Learning Model With Spark ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "historic-mobility",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainingFeaturesDf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11568/1197741066.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# Fit the pipeline to create a model from the training data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mlrPipelineModel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlrPipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainingFeaturesDf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mgetAccuracyForPipelineModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeaturesDf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'trainingFeaturesDf' is not defined"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Configure an machine learning pipeline, which consists of the \n",
    "# an estimator (classification) (Logistic regression)\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.01)\n",
    "lrPipeline = Pipeline(stages=[lr])\n",
    "\n",
    "# Fit the pipeline to create a model from the training data\n",
    "lrPipelineModel = lrPipeline.fit(trainingFeaturesDf)\n",
    "\n",
    "def getAccuracyForPipelineModel(featuresDf, model):\n",
    "    #perform prediction using the featuresdf and pipelineModel\n",
    "    #compute the accuracy in percentage float\n",
    "    results = model.transform(featuresDf)\n",
    "    labelsAndPreds = results.map(lambda p: (p.label, p.prediction))\n",
    "    return (calculateAccuracy(labelsAndPreds), results) \n",
    "\n",
    "# Evaluating the model on training data\n",
    "lrTrainAccuracy, lrTrainResultDf = getAccuracyForPipelineModel(trainingFeaturesDf, lrPipelineModel)\n",
    "\n",
    "# Repeat on test data\n",
    "lrTestAccuracy, lrTestResultDf = getAccuracyForPipelineModel(testFeaturesDf, lrPipelineModel)\n",
    "\n",
    "# Repeat on validation data\n",
    "lrValidationAccuracy, lrValidationResultDf = getAccuracyForPipelineModel(validationFeaturesDf, lrPipelineModel)\n",
    "\n",
    "print(\"==========================================\")\n",
    "print(\"LogisticRegression Model training accuracy (%) = \" + str(lrTrainAccuracy))\n",
    "print(\"LogisticRegression Model test accuracy (%) = \" + str(lrTestAccuracy))\n",
    "print(\"LogisticRegression Model validation accuracy (%) = \" + str(lrValidationAccuracy))\n",
    "print(\"==========================================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "first-karaoke",
   "metadata": {},
   "source": [
    "**Hyperparameter Tuning with Grid search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "material-cleveland",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lrValidationAccuracy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11568/2036045530.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mbestRegParam\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.01\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mbestModel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mbestAccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlrValidationAccuracy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'lrValidationAccuracy' is not defined"
     ]
    }
   ],
   "source": [
    "maxIterRange = [5, 10, 30, 50, 100]\n",
    "regParamRange = [1e-10, 1e-5, 1e-1]\n",
    "#baseline values from previous section\n",
    "bestIter = 10\n",
    "bestRegParam = 0.01\n",
    "bestModel = lr\n",
    "bestAccuracy = lrValidationAccuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "monetary-theta",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#for plotting purpose\n",
    "iterations = []\n",
    "regParams = []\n",
    "accuracies = []\n",
    "for maxIter in maxIterRange:\n",
    "    for rp in regParamRange:\n",
    "        currentLr = LogisticRegression(maxIter=maxIter, regParam=rp)\n",
    "        pipeline = Pipeline(stages=[currentLr])\n",
    "        model = pipeline.fit(trainingFeaturesDf)\n",
    "        \n",
    "        #use validation dataset test for accuracy\n",
    "        accuracy, resultDf = getAccuracyForPipelineModel(validationFeaturesDf, model)\n",
    "        print \"maxIter: %s, regParam: %s, accuracy: %s \" % (maxIter, rp, accuracy)\n",
    "        accuracies.append(accuracy)\n",
    "        regParams.append(log(rp))\n",
    "        iterations.append(maxIter)\n",
    "        \n",
    "        if accuracy > lrValidationAccuracy:\n",
    "            bestIter = maxIter\n",
    "            bestRegParam = rp\n",
    "            bestModel = model\n",
    "            bestAccuracy = accuracy\n",
    "\n",
    "\n",
    "print \"Best parameters: maxIter %s, regParam %s, accuracy : %s\" % (bestIter, bestRegParam, bestAccuracy)\n",
    "\n",
    "# Repeat on test data\n",
    "gridTestAccuracy, gridTestResultDf = getAccuracyForPipelineModel(testFeaturesDf, bestModel)\n",
    "\n",
    "print(\"==========================================\")\n",
    "print(\"Grid search Model test accuracy (%) = \" + str(gridTestAccuracy))\n",
    "print(\"==========================================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "broke-combination",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lrPipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11568/2888163258.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m# We now treat the Pipeline as an Estimator, wrapping it in a CrossValidator instance.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m# A CrossValidator requires an Estimator, a set of Estimator ParamMaps, and an Evaluator.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m crossValidator = CrossValidator(estimator=lrPipeline, \n\u001b[0m\u001b[0;32m     16\u001b[0m                                 \u001b[0mestimatorParamMaps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgrid\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m                                 \u001b[0mnumFolds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'lrPipeline' is not defined"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "# We use a ParamGridBuilder to construct a grid of parameters to search over.\n",
    "grid = (ParamGridBuilder()\n",
    "        .addGrid(lr.maxIter, maxIterRange) \n",
    "        .addGrid(lr.regParam,regParamRange )\n",
    "        .build())\n",
    "\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "\n",
    "# We now treat the Pipeline as an Estimator, wrapping it in a CrossValidator instance.\n",
    "# A CrossValidator requires an Estimator, a set of Estimator ParamMaps, and an Evaluator.\n",
    "crossValidator = CrossValidator(estimator=lrPipeline, \n",
    "                                estimatorParamMaps=grid, \n",
    "                                numFolds=5,\n",
    "                                evaluator=evaluator)\n",
    "\n",
    "\n",
    "# Run cross-validation, and choose the best model\n",
    "bestCvModel = crossValidator.fit(trainingFeaturesDf)\n",
    "\n",
    "# verify results on training dataset\n",
    "cvTrainAccuracy, cvTrainResultDf = getAccuracyForPipelineModel(trainingFeaturesDf, bestCvModel)\n",
    "\n",
    "# Repeat on test data\n",
    "cvTestAccuracy, cvTestResultDf = getAccuracyForPipelineModel(testFeaturesDf, bestCvModel)\n",
    "\n",
    "print(\"==========================================\")\n",
    "print(\"CV Model training accuracy (%) = \" + str(cvTrainAccuracy))\n",
    "print(\"CV Model test accuracy (%) = \" + str(cvTestAccuracy))\n",
    "print(\"==========================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "increased-bracket",
   "metadata": {},
   "source": [
    "# Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "novel-spirit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coefficients for the model\n",
    "linearModel.coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weekly-aside",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intercept for the model\n",
    "linearModel.intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "found-bouquet",
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff_df = pd.DataFrame({\"Feature\": [\"Intercept\"] + featureCols, \"Co-efficients\": np.insert(linearModel.coefficients.toArray(), 0, linearModel.intercept)})\n",
    "coeff_df = coeff_df[[\"Feature\", \"Co-efficients\"]]\n",
    "coeff_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
