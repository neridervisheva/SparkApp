{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "import sys, os\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
    "from pyspark.sql.functions import col, isnan, when, count, concat, lit, substring, udf, desc\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator, CrossValidatorModel\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Arguments from command line**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Arguments passed: -f C:\\Users\\nerim\\AppData\\Roaming\\jupyter\\runtime\\kernel-105f4721-4114-4fd1-a086-94d2b0aad1bf.json "
     ]
    }
   ],
   "source": [
    "print(\"\\nArguments passed:\", end = \" \")\n",
    "for i in range(1, len(sys.argv)):\n",
    "    print(sys.argv[i], end = \" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*python app.py --path ./Data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add the argument \"--path\" followed by the address where the data is located.\n"
     ]
    }
   ],
   "source": [
    "if '--path' in sys.argv:\n",
    "    dir_path = sys.argv[sys.argv.index('--path')+1]\n",
    "else:\n",
    "    print('Add the argument \"--path\" followed by the address where the data is located.')\n",
    "    exit(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We create our app**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.130:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>BigDataApp</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=BigDataApp>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"BigDataApp\").getOrCreate()\n",
    "spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOADING THE DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a schema for the DataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"Year\",IntegerType(),nullable=True),\n",
    "    StructField(\"Month\",IntegerType(),nullable=True),\n",
    "    StructField(\"DayofMonth\",IntegerType(),nullable=True),\n",
    "    StructField(\"DayOfWeek\",IntegerType(),nullable=True),\n",
    "    StructField(\"DepTime\",IntegerType(),nullable=True),\n",
    "    StructField(\"CRSDepTime\",IntegerType(),nullable=True),\n",
    "    StructField(\"ArrTime\",IntegerType(),nullable=True),\n",
    "    StructField(\"CRSArrTime\",IntegerType(),nullable=True),\n",
    "    StructField(\"UniqueCarrier\",StringType(),nullable=True),\n",
    "    StructField(\"FlightNum\",IntegerType(),nullable=True),\n",
    "    StructField(\"TailNum\",StringType(),nullable=True),\n",
    "    StructField(\"ActualElapsedTime\",IntegerType(),nullable=True),\n",
    "    StructField(\"CRSElapsedTime\",IntegerType(),nullable=True),\n",
    "    StructField(\"AirTime\",IntegerType(),nullable=True),\n",
    "    StructField(\"ArrDelay\",IntegerType(),nullable=True),\n",
    "    StructField(\"DepDelay\",IntegerType(),nullable=True),\n",
    "    StructField(\"Origin\",StringType(),nullable=True),\n",
    "    StructField(\"Dest\",StringType(),nullable=True),\n",
    "    StructField(\"Distance\",IntegerType(),nullable=True),\n",
    "    StructField(\"TaxiIn\",IntegerType(),nullable=True),\n",
    "    StructField(\"TaxiOut\",IntegerType(),nullable=True),\n",
    "    StructField(\"Cancelled\",IntegerType(),nullable=True),\n",
    "    StructField(\"CancellationCode\",StringType(),nullable=True),\n",
    "    StructField(\"Diverted\",IntegerType(),nullable=True),\n",
    "    StructField(\"CarrierDelay\",IntegerType(),nullable=True),\n",
    "    StructField(\"WeatherDelay\",IntegerType(),nullable=True),\n",
    "    StructField(\"NASDelay\",IntegerType(),nullable=True),\n",
    "    StructField(\"SecurityDelay\",IntegerType(),nullable=True),\n",
    "    StructField(\"LateAircraftDelay\",IntegerType(),nullable=True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load data into DataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "arg = './src/main/resources/data/2008.csv.bz2' #'./resources/data/2008.csv.bz2'\n",
    "dir_path = './src/main/resources/data' #'./resources/data'\n",
    "\n",
    "if dir_path[-1] != '/':\n",
    "    dir_path+='/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./src/main/resources/data/2008.csv.bz2']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files_path = []\n",
    "for path in os.listdir(dir_path):\n",
    "    # check if current path is a file\n",
    "    if os.path.isfile(os.path.join(dir_path, path)) and path.endswith(\".csv.bz2\"):\n",
    "        files_path.append(dir_path+path)\n",
    "        \n",
    "files_path#df1.union(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Year=2008, Month=1, DayofMonth=3, DayOfWeek=4, DepTime=1343, CRSDepTime=1325, ArrTime=1451, CRSArrTime=1435, UniqueCarrier='WN', FlightNum=588, TailNum='N240WN', ActualElapsedTime=68, CRSElapsedTime=70, AirTime=55, ArrDelay=16, DepDelay=18, Origin='HOU', Dest='LIT', Distance=393, TaxiIn=4, TaxiOut=9, Cancelled=0, CancellationCode=None, Diverted=0, CarrierDelay=16, WeatherDelay=0, NASDelay=0, SecurityDelay=0, LateAircraftDelay=0),\n",
       " Row(Year=2008, Month=1, DayofMonth=3, DayOfWeek=4, DepTime=1125, CRSDepTime=1120, ArrTime=1247, CRSArrTime=1245, UniqueCarrier='WN', FlightNum=1343, TailNum='N523SW', ActualElapsedTime=82, CRSElapsedTime=85, AirTime=71, ArrDelay=2, DepDelay=5, Origin='HOU', Dest='MAF', Distance=441, TaxiIn=3, TaxiOut=8, Cancelled=0, CancellationCode=None, Diverted=0, CarrierDelay=None, WeatherDelay=None, NASDelay=None, SecurityDelay=None, LateAircraftDelay=None),\n",
       " Row(Year=2008, Month=1, DayofMonth=3, DayOfWeek=4, DepTime=2009, CRSDepTime=2015, ArrTime=2136, CRSArrTime=2140, UniqueCarrier='WN', FlightNum=3841, TailNum='N280WN', ActualElapsedTime=87, CRSElapsedTime=85, AirTime=71, ArrDelay=-4, DepDelay=-6, Origin='HOU', Dest='MAF', Distance=441, TaxiIn=2, TaxiOut=14, Cancelled=0, CancellationCode=None, Diverted=0, CarrierDelay=None, WeatherDelay=None, NASDelay=None, SecurityDelay=None, LateAircraftDelay=None),\n",
       " Row(Year=2008, Month=1, DayofMonth=3, DayOfWeek=4, DepTime=903, CRSDepTime=855, ArrTime=1203, CRSArrTime=1205, UniqueCarrier='WN', FlightNum=3, TailNum='N308SA', ActualElapsedTime=120, CRSElapsedTime=130, AirTime=108, ArrDelay=-2, DepDelay=8, Origin='HOU', Dest='MCO', Distance=848, TaxiIn=5, TaxiOut=7, Cancelled=0, CancellationCode=None, Diverted=0, CarrierDelay=None, WeatherDelay=None, NASDelay=None, SecurityDelay=None, LateAircraftDelay=None),\n",
       " Row(Year=2008, Month=1, DayofMonth=3, DayOfWeek=4, DepTime=1423, CRSDepTime=1400, ArrTime=1726, CRSArrTime=1710, UniqueCarrier='WN', FlightNum=25, TailNum='N462WN', ActualElapsedTime=123, CRSElapsedTime=130, AirTime=107, ArrDelay=16, DepDelay=23, Origin='HOU', Dest='MCO', Distance=848, TaxiIn=6, TaxiOut=10, Cancelled=0, CancellationCode=None, Diverted=0, CarrierDelay=16, WeatherDelay=0, NASDelay=0, SecurityDelay=0, LateAircraftDelay=0)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = spark.read.csv(path=files_path[0], schema=schema, header=True)\n",
    "\n",
    "for f in files_path[1:]:\n",
    "    df = df.union(spark.read.csv(path=f, schema=schema, header=True))\n",
    "display(df.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remove forbidden variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(\"ArrTime\").drop(\"ActualElapsedTime\"\n",
    "        ).drop(\"AirTime\").drop(\"TaxiIn\").drop(\"Diverted\"\n",
    "        ).drop(\"CarrierDelay\").drop(\"WeatherDelay\").drop(\"NASDelay\"\n",
    "        ).drop(\"SecurityDelay\").drop(\"LateAircraftDelay\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- DayofMonth: integer (nullable = true)\n",
      " |-- DayOfWeek: integer (nullable = true)\n",
      " |-- DepTime: integer (nullable = true)\n",
      " |-- CRSDepTime: integer (nullable = true)\n",
      " |-- CRSArrTime: integer (nullable = true)\n",
      " |-- UniqueCarrier: string (nullable = true)\n",
      " |-- FlightNum: integer (nullable = true)\n",
      " |-- TailNum: string (nullable = true)\n",
      " |-- CRSElapsedTime: integer (nullable = true)\n",
      " |-- ArrDelay: integer (nullable = true)\n",
      " |-- DepDelay: integer (nullable = true)\n",
      " |-- Origin: string (nullable = true)\n",
      " |-- Dest: string (nullable = true)\n",
      " |-- Distance: integer (nullable = true)\n",
      " |-- TaxiOut: integer (nullable = true)\n",
      " |-- Cancelled: integer (nullable = true)\n",
      " |-- CancellationCode: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+---------+-------+----------+----------+-------------+---------+-------+--------------+--------+--------+------+----+--------+-------+---------+----------------+\n",
      "|Year|Month|DayofMonth|DayOfWeek|DepTime|CRSDepTime|CRSArrTime|UniqueCarrier|FlightNum|TailNum|CRSElapsedTime|ArrDelay|DepDelay|Origin|Dest|Distance|TaxiOut|Cancelled|CancellationCode|\n",
      "+----+-----+----------+---------+-------+----------+----------+-------------+---------+-------+--------------+--------+--------+------+----+--------+-------+---------+----------------+\n",
      "|2008|1    |3         |4        |1343   |1325      |1435      |WN           |588      |N240WN |70            |16      |18      |HOU   |LIT |393     |9      |0        |null            |\n",
      "|2008|1    |3         |4        |1125   |1120      |1245      |WN           |1343     |N523SW |85            |2       |5       |HOU   |MAF |441     |8      |0        |null            |\n",
      "|2008|1    |3         |4        |2009   |2015      |2140      |WN           |3841     |N280WN |85            |-4      |-6      |HOU   |MAF |441     |14     |0        |null            |\n",
      "|2008|1    |3         |4        |903    |855       |1205      |WN           |3        |N308SA |130           |-2      |8       |HOU   |MCO |848     |7      |0        |null            |\n",
      "|2008|1    |3         |4        |1423   |1400      |1710      |WN           |25       |N462WN |130           |16      |23      |HOU   |MCO |848     |10     |0        |null            |\n",
      "+----+-----+----------+---------+-------+----------+----------+-------------+---------+-------+--------------+--------+--------+------+----+--------+-------+---------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROCESSING THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2389217, 19)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df.count(), len(df.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unique values of the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "|Year|  count|\n",
      "+----+-------+\n",
      "|2008|2389217|\n",
      "+----+-------+\n",
      "\n",
      "None\n",
      "+-----+------+\n",
      "|Month| count|\n",
      "+-----+------+\n",
      "|    1|605765|\n",
      "|    2|569236|\n",
      "|    3|616090|\n",
      "|    4|598126|\n",
      "+-----+------+\n",
      "\n",
      "None\n",
      "+----------+-----+\n",
      "|DayofMonth|count|\n",
      "+----------+-----+\n",
      "|        31|40905|\n",
      "|        28|81974|\n",
      "|        26|74226|\n",
      "|        27|79527|\n",
      "|        12|73690|\n",
      "|        22|77730|\n",
      "|         1|76749|\n",
      "|        13|79091|\n",
      "|         6|79879|\n",
      "|        16|76898|\n",
      "|         3|81108|\n",
      "|        20|79565|\n",
      "|         5|75303|\n",
      "|        19|74270|\n",
      "|        15|77716|\n",
      "|         9|76023|\n",
      "|        17|80508|\n",
      "|         4|82101|\n",
      "|         8|77320|\n",
      "|        23|76860|\n",
      "+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n",
      "+---------+------+\n",
      "|DayOfWeek| count|\n",
      "+---------+------+\n",
      "|        1|347984|\n",
      "|        6|288097|\n",
      "|        3|365560|\n",
      "|        5|350566|\n",
      "|        4|349831|\n",
      "|        7|328237|\n",
      "|        2|358942|\n",
      "+---------+------+\n",
      "\n",
      "None\n",
      "+-------+-----+\n",
      "|DepTime|count|\n",
      "+-------+-----+\n",
      "|   1829| 2055|\n",
      "|   1238| 2056|\n",
      "|    833| 2240|\n",
      "|   1645| 3027|\n",
      "|   2142| 1201|\n",
      "|   1959| 1942|\n",
      "|   2122| 1591|\n",
      "|   1342| 2124|\n",
      "|    148|   21|\n",
      "|   1127| 2630|\n",
      "|   1522| 2240|\n",
      "|    623| 2046|\n",
      "|    737| 1970|\n",
      "|   2235|  758|\n",
      "|    858| 2800|\n",
      "|   1507| 2280|\n",
      "|   1721| 2158|\n",
      "|   1025| 3221|\n",
      "|    540|  676|\n",
      "|   1903| 2269|\n",
      "+-------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n",
      "+----------+-----+\n",
      "|CRSDepTime|count|\n",
      "+----------+-----+\n",
      "|      1645|10174|\n",
      "|      1959| 1510|\n",
      "|      1829|  621|\n",
      "|      1238|  727|\n",
      "|      1342|  780|\n",
      "|       833|  400|\n",
      "|      2122|  461|\n",
      "|      2142|  315|\n",
      "|       148|    1|\n",
      "|      1025|10095|\n",
      "|      2235| 1703|\n",
      "|      1522|  650|\n",
      "|       623|  167|\n",
      "|       540| 1827|\n",
      "|       858|  490|\n",
      "|      1127|  621|\n",
      "|      1721|  564|\n",
      "|      1507|  563|\n",
      "|       737|  117|\n",
      "|      1650|10161|\n",
      "+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n",
      "+----------+-----+\n",
      "|CRSArrTime|count|\n",
      "+----------+-----+\n",
      "|      1645| 6863|\n",
      "|      2142| 1279|\n",
      "|      1342| 1745|\n",
      "|      2122| 1373|\n",
      "|      1829| 2814|\n",
      "|       833|  832|\n",
      "|      1959| 2051|\n",
      "|      1238| 1373|\n",
      "|       148|   69|\n",
      "|      2235| 5395|\n",
      "|      1025| 6458|\n",
      "|       858|  849|\n",
      "|      1507| 1256|\n",
      "|      1127| 1230|\n",
      "|       737|  937|\n",
      "|      1721| 1401|\n",
      "|      1522| 1459|\n",
      "|       623|  350|\n",
      "|       540|  394|\n",
      "|      1650| 6798|\n",
      "+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n",
      "+-------------+------+\n",
      "|UniqueCarrier| count|\n",
      "+-------------+------+\n",
      "|           UA|154036|\n",
      "|           EV| 92383|\n",
      "|           DL|151409|\n",
      "|           OO|195184|\n",
      "|           YV| 85686|\n",
      "|           US|153539|\n",
      "|           OH| 71934|\n",
      "|           XE|138757|\n",
      "|           WN|398966|\n",
      "|           AA|204519|\n",
      "|           NW|122799|\n",
      "|           B6| 67478|\n",
      "|           F9| 30680|\n",
      "|           AQ|  7800|\n",
      "|           MQ|167860|\n",
      "|           HA| 18385|\n",
      "|           AS| 49885|\n",
      "|           FL| 86856|\n",
      "|           CO|102702|\n",
      "|           9E| 88359|\n",
      "+-------------+------+\n",
      "\n",
      "None\n",
      "+---------+-----+\n",
      "|FlightNum|count|\n",
      "+---------+-----+\n",
      "|     1580|  869|\n",
      "|      463|  857|\n",
      "|     2366|  200|\n",
      "|      496|  710|\n",
      "|     3918|  233|\n",
      "|      833|  413|\n",
      "|     1829|  434|\n",
      "|      148|  968|\n",
      "|     2866|  264|\n",
      "|      471| 1178|\n",
      "|     1238|  599|\n",
      "|     1342|  217|\n",
      "|     3749|  572|\n",
      "|     1088|  560|\n",
      "|     1591|  638|\n",
      "|     2122|  295|\n",
      "|     7754|  202|\n",
      "|     2659|  257|\n",
      "|     2142|  209|\n",
      "|     7240|   99|\n",
      "+---------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n",
      "+-------+-----+\n",
      "|TailNum|count|\n",
      "+-------+-----+\n",
      "| N672SW|  817|\n",
      "| N866AS|  608|\n",
      "| N466SW|  713|\n",
      "| N919UA|  544|\n",
      "| N516UA|  334|\n",
      "| N513UA|  369|\n",
      "| N102UW|  360|\n",
      "| N902DE|  560|\n",
      "|  N6700|  309|\n",
      "| N388DA|  303|\n",
      "| N251WN|  736|\n",
      "| N445WN|  770|\n",
      "| N496WN|  756|\n",
      "| N622SW|  607|\n",
      "| N13118|  399|\n",
      "| N10575|  600|\n",
      "| N517UA|  266|\n",
      "| N835UA|  389|\n",
      "| N318UA|  482|\n",
      "| N654AW|  453|\n",
      "+-------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n",
      "+--------------+-----+\n",
      "|CRSElapsedTime|count|\n",
      "+--------------+-----+\n",
      "|           148| 6717|\n",
      "|           471|   57|\n",
      "|           243| 1261|\n",
      "|           392|  261|\n",
      "|            31|  355|\n",
      "|            85|55667|\n",
      "|           137| 7524|\n",
      "|           251| 2127|\n",
      "|            65|60572|\n",
      "|           255| 4014|\n",
      "|            53| 5325|\n",
      "|           133| 7468|\n",
      "|           296|  844|\n",
      "|            78|13930|\n",
      "|           322| 1056|\n",
      "|           321|  757|\n",
      "|           362|  364|\n",
      "|           375| 1275|\n",
      "|           597|   61|\n",
      "|           155|27590|\n",
      "+--------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n",
      "+--------+-----+\n",
      "|ArrDelay|count|\n",
      "+--------+-----+\n",
      "|     148|  584|\n",
      "|     463|   10|\n",
      "|     -35| 1606|\n",
      "|     243|  108|\n",
      "|     392|   16|\n",
      "|      31| 9539|\n",
      "|     137|  699|\n",
      "|      85| 1999|\n",
      "|     251|  110|\n",
      "|     451|    5|\n",
      "|      65| 3137|\n",
      "|     458|    5|\n",
      "|      53| 4278|\n",
      "|     255|   90|\n",
      "|     133|  735|\n",
      "|     296|   44|\n",
      "|      78| 2317|\n",
      "|     322|   35|\n",
      "|     362|   24|\n",
      "|     321|   28|\n",
      "+--------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n",
      "+--------+-----+\n",
      "|DepDelay|count|\n",
      "+--------+-----+\n",
      "|     148|  505|\n",
      "|     496|    5|\n",
      "|     463|    7|\n",
      "|     243|  109|\n",
      "|     -35|   16|\n",
      "|     392|    5|\n",
      "|      31| 7452|\n",
      "|     516|    5|\n",
      "|      85| 1981|\n",
      "|     137|  637|\n",
      "|     251|   85|\n",
      "|     580|    2|\n",
      "|      65| 3295|\n",
      "|     458|    5|\n",
      "|      53| 3753|\n",
      "|     255|   99|\n",
      "|     133|  660|\n",
      "|     296|   47|\n",
      "|     472|    3|\n",
      "|      78| 1984|\n",
      "+--------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n",
      "+------+-----+\n",
      "|Origin|count|\n",
      "+------+-----+\n",
      "|   MSY|13782|\n",
      "|   GEG| 5390|\n",
      "|   SNA|16359|\n",
      "|   BUR|10784|\n",
      "|   GRB| 2775|\n",
      "|   GTF|  746|\n",
      "|   IDA| 1147|\n",
      "|   GRR| 5379|\n",
      "|   EUG| 2082|\n",
      "|   PVD| 7763|\n",
      "|   GSO| 4626|\n",
      "|   MYR| 1624|\n",
      "|   OAK|22620|\n",
      "|   MSN| 4419|\n",
      "|   COD|  363|\n",
      "|   BTM|  233|\n",
      "|   FAR| 1600|\n",
      "|   FSM|  937|\n",
      "|   DCA|29032|\n",
      "|   RFD|  225|\n",
      "+------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n",
      "+----+-----+\n",
      "|Dest|count|\n",
      "+----+-----+\n",
      "| MSY|13785|\n",
      "| GEG| 5384|\n",
      "| BUR|10785|\n",
      "| SNA|16367|\n",
      "| GRB| 2779|\n",
      "| GTF|  744|\n",
      "| IDA| 1154|\n",
      "| GRR| 5384|\n",
      "| EUG| 2083|\n",
      "| PVD| 7761|\n",
      "| GSO| 4626|\n",
      "| MYR| 1625|\n",
      "| OAK|22623|\n",
      "| MSN| 4412|\n",
      "| COD|  362|\n",
      "| BTM|  231|\n",
      "| FAR| 1598|\n",
      "| DCA|29044|\n",
      "| RFD|  229|\n",
      "| MLU|  903|\n",
      "+----+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n",
      "+--------+-----+\n",
      "|Distance|count|\n",
      "+--------+-----+\n",
      "|    1591| 1052|\n",
      "|     471|  600|\n",
      "|    1959|  242|\n",
      "|     833| 1845|\n",
      "|     496| 1396|\n",
      "|    1342|  717|\n",
      "|     148| 1570|\n",
      "|     463|  758|\n",
      "|    1829| 1069|\n",
      "|    1088|  720|\n",
      "|     737| 1935|\n",
      "|     540|  991|\n",
      "|     392|  960|\n",
      "|     623|  651|\n",
      "|    1522| 1656|\n",
      "|    1084|  633|\n",
      "|    1721| 3041|\n",
      "|    1460|  761|\n",
      "|    1618|  242|\n",
      "|    1352|   52|\n",
      "+--------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n",
      "+-------+-----+\n",
      "|TaxiOut|count|\n",
      "+-------+-----+\n",
      "|    148|   17|\n",
      "|     31|15457|\n",
      "|     85|  290|\n",
      "|    137|   34|\n",
      "|    251|    3|\n",
      "|     65|  855|\n",
      "|     53| 1987|\n",
      "|    133|   45|\n",
      "|     78|  411|\n",
      "|    108|  114|\n",
      "|    155|   14|\n",
      "|     34|11154|\n",
      "|    193|    2|\n",
      "|    101|  127|\n",
      "|    115|   76|\n",
      "|    126|   39|\n",
      "|     81|  350|\n",
      "|     28|22100|\n",
      "|    183|    4|\n",
      "|    300|    2|\n",
      "+-------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n",
      "+---------+-------+\n",
      "|Cancelled|  count|\n",
      "+---------+-------+\n",
      "|        1|  64442|\n",
      "|        0|2324775|\n",
      "+---------+-------+\n",
      "\n",
      "None\n",
      "+----------------+-------+\n",
      "|CancellationCode|  count|\n",
      "+----------------+-------+\n",
      "|            null|2324775|\n",
      "|               B|  25744|\n",
      "|               C|  12617|\n",
      "|               A|  26075|\n",
      "|               D|      6|\n",
      "+----------------+-------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "for c in df.columns:\n",
    "    print(df.groupBy(c).count().show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if Year is same as title of csv\n",
    "# check Month in 1-12 interval\n",
    "# check DayofMonth in 1-31 interval\n",
    "# check DayOfWeek in 1-7 interval\n",
    "# Que es UniqueCarrier?\n",
    "# CRSElapsedTime = CRSArrTime - CRSDepTime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remove duplicated rows**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+---------+-------+----------+----------+-------------+---------+-------+--------------+--------+--------+------+----+--------+-------+---------+----------------+-----+\n",
      "|Year|Month|DayofMonth|DayOfWeek|DepTime|CRSDepTime|CRSArrTime|UniqueCarrier|FlightNum|TailNum|CRSElapsedTime|ArrDelay|DepDelay|Origin|Dest|Distance|TaxiOut|Cancelled|CancellationCode|count|\n",
      "+----+-----+----------+---------+-------+----------+----------+-------------+---------+-------+--------------+--------+--------+------+----+--------+-------+---------+----------------+-----+\n",
      "+----+-----+----------+---------+-------+----------+----------+-------------+---------+-------+--------------+--------+--------+------+----+--------+-------+---------+----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(df.columns).count().filter(\"count > 1\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remove instances of cancelled flights**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+\n",
      "|Cancelled|  count|\n",
      "+---------+-------+\n",
      "|        1|  64442|\n",
      "|        0|2324771|\n",
      "+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('Cancelled').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------+\n",
      "|CancellationCode|  count|\n",
      "+----------------+-------+\n",
      "|            null|2324771|\n",
      "|               B|  25744|\n",
      "|               C|  12617|\n",
      "|               A|  26075|\n",
      "|               D|      6|\n",
      "+----------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('CancellationCode').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.filter(df.Cancelled == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.filter(df.CancellationCode.isNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('CancellationCode', 'Cancelled')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analyze missing values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name                Count %\n",
      "--------------  -----------\n",
      "Year            0\n",
      "Month           0\n",
      "DayofMonth      0\n",
      "DayOfWeek       0\n",
      "DepTime         0\n",
      "CRSDepTime      0\n",
      "CRSArrTime      0\n",
      "UniqueCarrier   0\n",
      "FlightNum       0\n",
      "TailNum         1.29045e-06\n",
      "CRSElapsedTime  0.000123883\n",
      "ArrDelay        0.00243207\n",
      "DepDelay        0\n",
      "Origin          0\n",
      "Dest            0\n",
      "Distance        0\n",
      "TaxiOut         0\n"
     ]
    }
   ],
   "source": [
    "print(tabulate([[c, df.filter(col(c).isNull()).count()/df.count()] for c in df.columns], headers=['Name', 'Count %']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TailNum and CRSElapsedTime can not be imputed from any other column, and ArrDelay is the target variable, but their number of missing values is not significant taking into account the total number, so the rows containing those missing values are removed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2319115, 17)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df.count(), len(df.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## !!!!!Date preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|      Date|count|\n",
      "+----------+-----+\n",
      "|04/05/2008|16939|\n",
      "|03/28/2008|20534|\n",
      "|01/15/2008|19167|\n",
      "|04/24/2008|20524|\n",
      "|01/08/2008|19104|\n",
      "|03/29/2008|17168|\n",
      "|03/02/2008|19095|\n",
      "|03/14/2008|20591|\n",
      "|04/13/2008|19442|\n",
      "|01/13/2008|18549|\n",
      "|03/27/2008|19997|\n",
      "|02/10/2008|18438|\n",
      "|04/23/2008|20320|\n",
      "|03/11/2008|19882|\n",
      "|02/05/2008|18322|\n",
      "|01/24/2008|19899|\n",
      "|04/30/2008|20311|\n",
      "|03/08/2008|15670|\n",
      "|03/23/2008|19309|\n",
      "|04/29/2008|19964|\n",
      "+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\n",
    "    \"Date\", F.date_format(F.expr(\"make_date(Year, Month, DayofMonth)\"), \"MM/dd/yyyy\")\n",
    ")\n",
    "\n",
    "df.groupBy('Date').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+\n",
      "|Season|  count|\n",
      "+------+-------+\n",
      "|     1|1185064|\n",
      "|     4|1134051|\n",
      "+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\n",
    "    \"Season\", when((df.Month>2) & (df.Month<6), 1).when((df.Month>5) & (df.Month<9), 2\n",
    "        ).when((df.Month>8) & (df.Month<12), 3).otherwise(4)\n",
    ")\n",
    "df.groupBy('Season').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check that DepDelay = DepTime - CRSDepTime**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"DepTimeNew\", when(F.length(df.DepTime) == 3, concat(lit(\"0\"),df.DepTime)) \\\n",
    "        .when(F.length(df.DepTime) == 2, concat(lit(\"00\"),df.DepTime)) \\\n",
    "        .otherwise(df.DepTime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"CRSDepTimeNew\", when(F.length(df.CRSDepTime) == 3, concat(lit(\"0\"),df.CRSDepTime)) \\\n",
    "        .when(F.length(df.CRSDepTime) == 2, concat(lit(\"00\"),df.CRSDepTime)) \\\n",
    "        .otherwise(df.CRSDepTime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"CRSArrTimeNew\", when(F.length(df.CRSArrTime) == 3, concat(lit(\"0\"),df.CRSArrTime)) \\\n",
    "        .when(F.length(df.CRSArrTime) == 2, concat(lit(\"00\"),df.CRSArrTime)) \\\n",
    "        .otherwise(df.CRSArrTime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----------+-------------+----------+-------------+\n",
      "|DepTime|DepTimeNew|CRSDepTime|CRSDepTimeNew|CRSArrTime|CRSArrTimeNew|\n",
      "+-------+----------+----------+-------------+----------+-------------+\n",
      "|    813|      0813|       810|         0810|       925|         0925|\n",
      "|   1210|      1210|      1200|         1200|      1325|         1325|\n",
      "|   2154|      2154|      1829|         1829|      1930|         1930|\n",
      "|   1940|      1940|      1950|         1950|      2108|         2108|\n",
      "|   2300|      2300|      2005|         2005|      2055|         2055|\n",
      "|   2045|      2045|      2019|         2019|      2212|         2212|\n",
      "|    607|      0607|       600|         0600|       706|         0706|\n",
      "|    747|      0747|       750|         0750|       955|         0955|\n",
      "|   1155|      1155|      1155|         1155|      1249|         1249|\n",
      "|   1224|      1224|      1225|         1225|      1330|         1330|\n",
      "|   1225|      1225|      1235|         1235|      1309|         1309|\n",
      "|   1417|      1417|      1415|         1415|      1510|         1510|\n",
      "|   1451|      1451|      1455|         1455|      1635|         1635|\n",
      "|   1558|      1558|      1600|         1600|      1725|         1725|\n",
      "|   1739|      1739|      1730|         1730|      1845|         1845|\n",
      "|   1757|      1757|      1800|         1800|      1915|         1915|\n",
      "|   1904|      1904|      1855|         1855|      2056|         2056|\n",
      "|   2016|      2016|      2020|         2020|      2245|         2245|\n",
      "|   2044|      2044|      2050|         2050|      2315|         2315|\n",
      "|   2116|      2116|      2120|         2120|      2135|         2135|\n",
      "+-------+----------+----------+-------------+----------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"DepTime\",\"DepTimeNew\",\"CRSDepTime\",\"CRSDepTimeNew\",\"CRSArrTime\",\"CRSArrTimeNew\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+--------+\n",
      "|DepTimeNew|CRSDepTimeNew|DepDelay|\n",
      "+----------+-------------+--------+\n",
      "|      0813|         0810|       3|\n",
      "|      1210|         1200|      10|\n",
      "|      2154|         1829|     205|\n",
      "|      1940|         1950|     -10|\n",
      "|      2300|         2005|     175|\n",
      "|      2045|         2019|      26|\n",
      "|      0607|         0600|       7|\n",
      "|      0747|         0750|      -3|\n",
      "|      1155|         1155|       0|\n",
      "|      1224|         1225|      -1|\n",
      "|      1225|         1235|     -10|\n",
      "|      1417|         1415|       2|\n",
      "|      1451|         1455|      -4|\n",
      "|      1558|         1600|      -2|\n",
      "|      1739|         1730|       9|\n",
      "|      1757|         1800|      -3|\n",
      "|      1904|         1855|       9|\n",
      "|      2016|         2020|      -4|\n",
      "|      2044|         2050|      -6|\n",
      "|      2116|         2120|      -4|\n",
      "+----------+-------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"DepTimeNew\",\"CRSDepTimeNew\",\"DepDelay\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"CRSDepTimeNewHour\", substring(df.CRSDepTimeNew, 1,2)) \\\n",
    "    .withColumn(\"CRSDepTimeNewMinute\", substring(df.CRSDepTimeNew, 3,2)) \\\n",
    "    .withColumn(\"DepTimeNewHour\", substring(df.DepTimeNew, 1,2)) \\\n",
    "    .withColumn(\"DepTimeNewMinute\", substring(df.DepTimeNew, 3,2))\n",
    "\n",
    "df = df.withColumn(\"CRSArrTimeNewHour\", substring(df.CRSArrTimeNew, 1,2)) \\\n",
    "    .withColumn(\"CRSArrTimeNewMinute\", substring(df.CRSArrTimeNew, 3,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------------+-------------------+----------+--------------+----------------+\n",
      "|CRSDepTimeNew|CRSDepTimeNewHour|CRSDepTimeNewMinute|DepTimeNew|DepTimeNewHour|DepTimeNewMinute|\n",
      "+-------------+-----------------+-------------------+----------+--------------+----------------+\n",
      "|         0810|               08|                 10|      0813|            08|              13|\n",
      "|         1200|               12|                 00|      1210|            12|              10|\n",
      "|         1829|               18|                 29|      2154|            21|              54|\n",
      "|         1950|               19|                 50|      1940|            19|              40|\n",
      "|         2005|               20|                 05|      2300|            23|              00|\n",
      "|         2019|               20|                 19|      2045|            20|              45|\n",
      "|         0600|               06|                 00|      0607|            06|              07|\n",
      "|         0750|               07|                 50|      0747|            07|              47|\n",
      "|         1155|               11|                 55|      1155|            11|              55|\n",
      "|         1225|               12|                 25|      1224|            12|              24|\n",
      "|         1235|               12|                 35|      1225|            12|              25|\n",
      "|         1415|               14|                 15|      1417|            14|              17|\n",
      "|         1455|               14|                 55|      1451|            14|              51|\n",
      "|         1600|               16|                 00|      1558|            15|              58|\n",
      "|         1730|               17|                 30|      1739|            17|              39|\n",
      "|         1800|               18|                 00|      1757|            17|              57|\n",
      "|         1855|               18|                 55|      1904|            19|              04|\n",
      "|         2020|               20|                 20|      2016|            20|              16|\n",
      "|         2050|               20|                 50|      2044|            20|              44|\n",
      "|         2120|               21|                 20|      2116|            21|              16|\n",
      "+-------------+-----------------+-------------------+----------+--------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"CRSDepTimeNew\",\"CRSDepTimeNewHour\",\"CRSDepTimeNewMinute\",\"DepTimeNew\",\"DepTimeNewHour\",\"DepTimeNewMinute\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|        datetime|\n",
      "+----------------+\n",
      "|07/01/2008 08:10|\n",
      "|03/01/2008 12:00|\n",
      "|02/01/2008 18:29|\n",
      "|04/01/2008 19:50|\n",
      "|31/01/2008 20:05|\n",
      "|03/01/2008 20:19|\n",
      "|27/01/2008 06:00|\n",
      "|13/01/2008 07:50|\n",
      "|09/01/2008 11:55|\n",
      "|29/01/2008 12:25|\n",
      "|12/01/2008 12:35|\n",
      "|11/01/2008 14:15|\n",
      "|09/01/2008 14:55|\n",
      "|28/01/2008 16:00|\n",
      "|28/01/2008 17:30|\n",
      "|20/01/2008 18:00|\n",
      "|23/01/2008 18:55|\n",
      "|22/01/2008 20:20|\n",
      "|29/01/2008 20:50|\n",
      "|30/01/2008 21:20|\n",
      "+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\n",
    "    \"datetime\",\n",
    "    F.date_format(\n",
    "        F.expr(\"make_timestamp(Year, Month, DayofMonth, CRSDepTimeNewHour, CRSDepTimeNewMinute, 0)\"),\n",
    "        \"dd/MM/yyyy HH:mm\"\n",
    "    )\n",
    ")\n",
    "\n",
    "df.select(\"datetime\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   !!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.withColumn(\"DepDelayNew\", when(abs((col(\"DepTimeNewHour\").cast('long')*60 + col(\"DepTimeNewMinute\")) - (col(\"CRSDepTimeNewHour\").cast('long')*60 + col(\"CRSDepTimeNewMinute\"))) < abs((col(\"CRSDepTimeNewHour\").cast('long')*60 + col(\"CRSDepTimeNewMinute\")) - (col(\"DepTimeNewHour\").cast('long')*60 + col(\"DepTimeNewMinute\"))), \n",
    "#    (col(\"DepTimeNewHour\").cast('long')*60 + col(\"DepTimeNewMinute\")) - (col(\"CRSDepTimeNewHour\").cast('long')*60 + col(\"CRSDepTimeNewMinute\"))) \\\n",
    "#    .otherwise((col(\"CRSDepTimeNewHour\").cast('long')*60 + col(\"CRSDepTimeNewMinute\")) - (col(\"DepTimeNewHour\").cast('long')*60 + col(\"DepTimeNewMinute\"))))\n",
    "                   \n",
    "df = df.withColumn(\"Duration\", \n",
    "    (col(\"CRSArrTimeNewHour\").cast('long')*60 + col(\"CRSArrTimeNewMinute\").cast('long')) - (col(\"CRSDepTimeNewHour\").cast('long')*60 + col(\"CRSDepTimeNewMinute\").cast('long')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.withColumn(\"DepDelayNew\",col(\"DepDelayNew\").cast(IntegerType()))\n",
    "df = df.withColumn(\"Duration\",col(\"Duration\").cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.select(\"DepTimeNew\",\"CRSDepTimeNew\",\"DepDelay\",\"DepDelayNew\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+--------+--------------+\n",
      "|CRSArrTimeNew|CRSDepTimeNew|Duration|CRSElapsedTime|\n",
      "+-------------+-------------+--------+--------------+\n",
      "|         0925|         0810|      75|            75|\n",
      "|         1325|         1200|      85|            85|\n",
      "|         1930|         1829|      61|            61|\n",
      "|         2108|         1950|      78|           138|\n",
      "|         2055|         2005|      50|            50|\n",
      "|         2212|         2019|     113|           173|\n",
      "|         0706|         0600|      66|            66|\n",
      "|         0955|         0750|     125|           125|\n",
      "|         1249|         1155|      54|            54|\n",
      "|         1330|         1225|      65|            65|\n",
      "|         1309|         1235|      34|            34|\n",
      "|         1510|         1415|      55|           115|\n",
      "|         1635|         1455|     100|           160|\n",
      "|         1725|         1600|      85|            85|\n",
      "|         1845|         1730|      75|            75|\n",
      "|         1915|         1800|      75|            75|\n",
      "|         2056|         1855|     121|           121|\n",
      "|         2245|         2020|     145|            85|\n",
      "|         2315|         2050|     145|            85|\n",
      "|         2135|         2120|      15|            75|\n",
      "+-------------+-------------+--------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"CRSArrTimeNew\",\"CRSDepTimeNew\",\"Duration\",\"CRSElapsedTime\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(\"CRSDepTime\",'CRSDepTimeNew','CRSDepTimeNewHour','CRSDepTimeNewMinute',\"CRSArrTime\",'CRSArrTimeNew',\n",
    "             'CRSArrTimeNewHour', 'CRSArrTimeNewMinute',\"DepTime\", 'DepTimeNew', 'DepTimeNewHour',  'DepTimeNewMinute', \n",
    "             'DepDelayNew')\n",
    "# por que no se deja nada de CRSDepTime, CRSArrTime y DepTime?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concordancy between related variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**No flights with same Origin and Destination**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.filter(df.Origin != df.Dest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- DayofMonth: integer (nullable = true)\n",
      " |-- DayOfWeek: integer (nullable = true)\n",
      " |-- UniqueCarrier: string (nullable = true)\n",
      " |-- FlightNum: integer (nullable = true)\n",
      " |-- TailNum: string (nullable = true)\n",
      " |-- CRSElapsedTime: integer (nullable = true)\n",
      " |-- ArrDelay: integer (nullable = true)\n",
      " |-- DepDelay: integer (nullable = true)\n",
      " |-- Origin: string (nullable = true)\n",
      " |-- Dest: string (nullable = true)\n",
      " |-- Distance: integer (nullable = true)\n",
      " |-- TaxiOut: integer (nullable = true)\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Season: integer (nullable = false)\n",
      " |-- datetime: string (nullable = true)\n",
      " |-- Duration: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name              Count %\n",
      "--------------  ---------\n",
      "Year                    0\n",
      "Month                   0\n",
      "DayofMonth              0\n",
      "DayOfWeek               0\n",
      "UniqueCarrier           0\n",
      "FlightNum               0\n",
      "TailNum                 0\n",
      "CRSElapsedTime          0\n",
      "ArrDelay                0\n",
      "DepDelay                0\n",
      "Origin                  0\n",
      "Dest                    0\n",
      "Distance                0\n",
      "TaxiOut                 0\n",
      "Date                    0\n",
      "Season                  0\n",
      "datetime               32\n",
      "Duration             3879\n"
     ]
    }
   ],
   "source": [
    "print(tabulate([[c, df.filter(col(c).isNull()).count()] for c in df.columns], headers=['Name', 'Count %']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+---------+-------------+---------+-------+--------------+--------+--------+------+----+--------+-------+----------+------+----------------+--------+\n",
      "|Year|Month|DayofMonth|DayOfWeek|UniqueCarrier|FlightNum|TailNum|CRSElapsedTime|ArrDelay|DepDelay|Origin|Dest|Distance|TaxiOut|      Date|Season|        datetime|Duration|\n",
      "+----+-----+----------+---------+-------------+---------+-------+--------------+--------+--------+------+----+--------+-------+----------+------+----------------+--------+\n",
      "|2008|    1|        14|        1|           WN|      904| N650SW|           105|       1|      -1|   LAX| SLC|     590|     12|01/14/2008|     4|14/01/2008 21:20|    null|\n",
      "|2008|    1|        28|        1|           WN|       61| N700GS|           290|      19|      31|   LAS| MHT|    2356|     23|01/28/2008|     4|28/01/2008 16:15|    null|\n",
      "|2008|    1|        22|        2|           DL|     1586| N373DA|           142|       1|      12|   ATL| JFK|     760|     27|01/22/2008|     4|22/01/2008 21:40|    null|\n",
      "|2008|    1|         5|        6|           YV|     2881| N77286|            73|      -3|       0|   PHX| ELP|     347|     15|01/05/2008|     4|05/01/2008 22:50|    null|\n",
      "|2008|    1|        16|        3|           US|      959| N203UW|           118|      46|      55|   CLT| FLL|     631|     15|01/16/2008|     4|16/01/2008 22:05|    null|\n",
      "+----+-----+----------+---------+-------------+---------+-------+--------------+--------+--------+------+----+--------+-------+----------+------+----------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.Duration.isNull()).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "imputer = Imputer(\n",
    "    inputCols=['Duration'], \n",
    "    outputCols=['Duration']\n",
    ")\n",
    "\n",
    "df = imputer.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fix format of variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('Date',  'datetime', 'FlightNum', 'TailNum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Year',\n",
       " 'Month',\n",
       " 'DayofMonth',\n",
       " 'DayOfWeek',\n",
       " 'UniqueCarrier',\n",
       " 'CRSElapsedTime',\n",
       " 'ArrDelay',\n",
       " 'DepDelay',\n",
       " 'Origin',\n",
       " 'Dest',\n",
       " 'Distance',\n",
       " 'TaxiOut',\n",
       " 'Season',\n",
       " 'Duration']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+---------+-------------+--------------+--------+--------+------+----+--------+-------+------+--------+\n",
      "|Year|Month|DayofMonth|DayOfWeek|UniqueCarrier|CRSElapsedTime|ArrDelay|DepDelay|Origin|Dest|Distance|TaxiOut|Season|Duration|\n",
      "+----+-----+----------+---------+-------------+--------------+--------+--------+------+----+--------+-------+------+--------+\n",
      "|2008|    1|         7|        1|           WN|            75|      -9|       3|   BUR| OAK|     325|      4|     4|      75|\n",
      "|2008|    1|         3|        4|           WN|            85|       2|      10|   MSY| DAL|     437|      4|     4|      85|\n",
      "|2008|    1|         2|        3|           YV|            61|     195|     205|   SPI| ORD|     174|      4|     4|      61|\n",
      "|2008|    1|         4|        5|           OO|           138|     -12|     -10|   BDL| MKE|     780|      4|     4|      78|\n",
      "|2008|    1|        31|        4|           WN|            50|     169|     175|   HRL| SAT|     233|      4|     4|      50|\n",
      "+----+-----+----------+---------+-------------+--------------+--------+--------+------+----+--------+-------+------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CREATING THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|            features|ArrDelay|\n",
      "+--------------------+--------+\n",
      "|[2008.0,1.0,7.0,1...|      -9|\n",
      "|[2008.0,1.0,3.0,4...|       2|\n",
      "|[2008.0,1.0,2.0,3...|     195|\n",
      "+--------------------+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "training = df.drop( 'Date',  'datetime', 'FlightNum', 'TailNum', 'UniqueCarrier', 'Origin', 'Dest')\n",
    "training = training.select('Year', 'Month', 'DayofMonth', 'DayOfWeek', 'CRSElapsedTime',\n",
    " 'DepDelay', 'Distance', 'TaxiOut', 'Season', 'Duration', 'ArrDelay')\n",
    "\n",
    "vectorAssembler = VectorAssembler(inputCols = ['Year', 'Month', 'DayofMonth', 'DayOfWeek', 'CRSElapsedTime',\n",
    " 'DepDelay', 'Distance', 'TaxiOut', 'Season', 'Duration'], outputCol = 'features')\n",
    "vdf = vectorAssembler.transform(training)\n",
    "vdf = vdf.select(['features', 'ArrDelay'])\n",
    "vdf.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+---------+--------------+--------+--------+-------+------+--------+--------+\n",
      "|Year|Month|DayofMonth|DayOfWeek|CRSElapsedTime|DepDelay|Distance|TaxiOut|Season|Duration|ArrDelay|\n",
      "+----+-----+----------+---------+--------------+--------+--------+-------+------+--------+--------+\n",
      "|2008|    1|         7|        1|            75|       3|     325|      4|     4|      75|      -9|\n",
      "|2008|    1|         3|        4|            85|      10|     437|      4|     4|      85|       2|\n",
      "|2008|    1|         2|        3|            61|     205|     174|      4|     4|      61|     195|\n",
      "|2008|    1|         4|        5|           138|     -10|     780|      4|     4|      78|     -12|\n",
      "|2008|    1|        31|        4|            50|     175|     233|      4|     4|      50|     169|\n",
      "+----+-----+----------+---------+--------------+--------+--------+-------+------+--------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = vdf.randomSplit([0.7, 0.3])\n",
    "train_df = splits[0]\n",
    "test_df = splits[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression(featuresCol = 'features', labelCol='ArrDelay', maxIter=10, regParam=0.3, elasticNetParam=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [0.0,0.0,0.0,0.0,-0.02741521907070583,0.9865102087300102,0.0,0.7979935767670143,0.0,0.0]\n",
      "Intercept: -10.72275178717567\n"
     ]
    }
   ],
   "source": [
    "lr_model = lr.fit(train_df)\n",
    "print(\"Coefficients: \" + str(lr_model.coefficients))\n",
    "print(\"Intercept: \" + str(lr_model.intercept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 10.937266\n",
      "r2: 0.923502\n"
     ]
    }
   ],
   "source": [
    "trainingSummary = lr_model.summary\n",
    "print(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)\n",
    "print(\"r2: %f\" % trainingSummary.r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 370.0 failed 1 times, most recent failure: Lost task 0.0 in stage 370.0 (TID 1079) (192.168.1.130 executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)\r\n\tat java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:551)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:519)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n\t... 14 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\r\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:166)\r\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)\r\n\tat java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:551)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:519)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n\t... 14 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-56-a54e28ea1c6c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m training = training.select('Year', 'Month', 'DayofMonth', 'DayOfWeek', 'CRSElapsedTime',\n\u001b[0;32m      4\u001b[0m  'DepDelay', 'Distance', 'TaxiOut', 'Season', 'Duration', 'ArrDelay')\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mtrainingData\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mVectors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoDF\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"features\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"label\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\py37machlearn\\lib\\site-packages\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36mtoDF\u001b[1;34m(self, schema, sampleRatio)\u001b[0m\n\u001b[0;32m    100\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Alice'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m         \"\"\"\n\u001b[1;32m--> 102\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msparkSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msampleRatio\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m     \u001b[0mRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoDF\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtoDF\u001b[0m  \u001b[1;31m# type: ignore[assignment]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\py37machlearn\\lib\\site-packages\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m    893\u001b[0m             )\n\u001b[0;32m    894\u001b[0m         return self._create_dataframe(\n\u001b[1;32m--> 895\u001b[1;33m             \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverifySchema\u001b[0m  \u001b[1;31m# type: ignore[arg-type]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    896\u001b[0m         )\n\u001b[0;32m    897\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\py37machlearn\\lib\\site-packages\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36m_create_dataframe\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m    932\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    933\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 934\u001b[1;33m             \u001b[0mrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstruct\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_createFromRDD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    935\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    936\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstruct\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\py37machlearn\\lib\\site-packages\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36m_createFromRDD\u001b[1;34m(self, rdd, schema, samplingRatio)\u001b[0m\n\u001b[0;32m    598\u001b[0m         \"\"\"\n\u001b[0;32m    599\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mschema\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 600\u001b[1;33m             \u001b[0mstruct\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inferSchema\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mschema\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    601\u001b[0m             \u001b[0mconverter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_create_converter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstruct\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    602\u001b[0m             \u001b[0mtupled_rdd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconverter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\py37machlearn\\lib\\site-packages\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36m_inferSchema\u001b[1;34m(self, rdd, samplingRatio, names)\u001b[0m\n\u001b[0;32m    544\u001b[0m         \u001b[1;33m:\u001b[0m\u001b[1;32mclass\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStructType\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    545\u001b[0m         \"\"\"\n\u001b[1;32m--> 546\u001b[1;33m         \u001b[0mfirst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    547\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mfirst\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    548\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"The first row in RDD is empty, \"\u001b[0m \u001b[1;34m\"can not infer schema\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\py37machlearn\\lib\\site-packages\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mfirst\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1901\u001b[0m         \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mRDD\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mempty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1902\u001b[0m         \"\"\"\n\u001b[1;32m-> 1903\u001b[1;33m         \u001b[0mrs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1904\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1905\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mrs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\py37machlearn\\lib\\site-packages\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mtake\u001b[1;34m(self, num)\u001b[0m\n\u001b[0;32m   1881\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1882\u001b[0m             \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1883\u001b[1;33m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1884\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1885\u001b[0m             \u001b[0mitems\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\py37machlearn\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36mrunJob\u001b[1;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[0;32m   1484\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1485\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1486\u001b[1;33m         \u001b[0msock_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1487\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1488\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\py37machlearn\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1322\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1324\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\py37machlearn\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    188\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 190\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    191\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\py37machlearn\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 370.0 failed 1 times, most recent failure: Lost task 0.0 in stage 370.0 (TID 1079) (192.168.1.130 executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)\r\n\tat java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:551)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:519)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n\t... 14 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\r\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:166)\r\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)\r\n\tat java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:551)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:519)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n\t... 14 more\r\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "training = df.drop( 'Date',  'datetime', 'FlightNum', 'TailNum', 'UniqueCarrier', 'Origin', 'Dest')\n",
    "training = training.select('Year', 'Month', 'DayofMonth', 'DayOfWeek', 'CRSElapsedTime',\n",
    " 'DepDelay', 'Distance', 'TaxiOut', 'Season', 'Duration', 'ArrDelay')\n",
    "trainingData=training.rdd.map(lambda x:(Vectors.dense(x[0:-1]), x[-1])).toDF([\"features\", \"label\"])\n",
    "\n",
    "\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "\n",
    "# Fit the model\n",
    "lrModel = lr.fit(trainingData)\n",
    "\n",
    "# Print the coefficients and intercept for logistic regression\n",
    "print(\"Coefficients: \" + str(lrModel.coefficients))\n",
    "print(\"Intercept: \" + str(lrModel.intercept))\n",
    "\n",
    "# We can also use the multinomial family for binary classification\n",
    "mlr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8, family=\"multinomial\")\n",
    "\n",
    "# Fit the model\n",
    "mlrModel = mlr.fit(training)\n",
    "\n",
    "# Print the coefficients and intercepts for logistic regression with multinomial family\n",
    "print(\"Multinomial coefficients: \" + str(mlrModel.coefficientMatrix))\n",
    "print(\"Multinomial intercepts: \" + str(mlrModel.interceptVector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o550.randomSplit.\n: java.lang.ClassCastException: class java.lang.Integer cannot be cast to class java.lang.Double (java.lang.Integer and java.lang.Double are in module java.base of loader 'bootstrap')\r\n\tat scala.runtime.BoxesRunTime.unboxToDouble(BoxesRunTime.java:116)\r\n\tat scala.runtime.ScalaRunTime$.array_update(ScalaRunTime.scala:76)\r\n\tat scala.collection.IterableLike.copyToArray(IterableLike.scala:256)\r\n\tat scala.collection.IterableLike.copyToArray$(IterableLike.scala:251)\r\n\tat scala.collection.AbstractIterable.copyToArray(Iterable.scala:56)\r\n\tat scala.collection.TraversableOnce.copyToArray(TraversableOnce.scala:334)\r\n\tat scala.collection.TraversableOnce.copyToArray$(TraversableOnce.scala:333)\r\n\tat scala.collection.AbstractTraversable.copyToArray(Traversable.scala:108)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:342)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\r\n\tat scala.collection.AbstractTraversable.toArray(Traversable.scala:108)\r\n\tat org.apache.spark.sql.Dataset.randomSplit(Dataset.scala:2378)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-58-18ce9555f35c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# split into training(60%), validation(20%) and test(20%) datasets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrainingDf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidationDf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestDf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandomSplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#print(trainingDf.take(1))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\py37machlearn\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mrandomSplit\u001b[1;34m(self, weights, seed)\u001b[0m\n\u001b[0;32m   1364\u001b[0m         \u001b[0mseed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mseed\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mseed\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaxsize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1365\u001b[0m         df_array = self._jdf.randomSplit(\n\u001b[1;32m-> 1366\u001b[1;33m             \u001b[0m_to_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"ColumnOrName\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1367\u001b[0m         )\n\u001b[0;32m   1368\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdf_array\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\py37machlearn\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1322\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1324\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\py37machlearn\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    188\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 190\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    191\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\py37machlearn\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o550.randomSplit.\n: java.lang.ClassCastException: class java.lang.Integer cannot be cast to class java.lang.Double (java.lang.Integer and java.lang.Double are in module java.base of loader 'bootstrap')\r\n\tat scala.runtime.BoxesRunTime.unboxToDouble(BoxesRunTime.java:116)\r\n\tat scala.runtime.ScalaRunTime$.array_update(ScalaRunTime.scala:76)\r\n\tat scala.collection.IterableLike.copyToArray(IterableLike.scala:256)\r\n\tat scala.collection.IterableLike.copyToArray$(IterableLike.scala:251)\r\n\tat scala.collection.AbstractIterable.copyToArray(Iterable.scala:56)\r\n\tat scala.collection.TraversableOnce.copyToArray(TraversableOnce.scala:334)\r\n\tat scala.collection.TraversableOnce.copyToArray$(TraversableOnce.scala:333)\r\n\tat scala.collection.AbstractTraversable.copyToArray(Traversable.scala:108)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:342)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\r\n\tat scala.collection.AbstractTraversable.toArray(Traversable.scala:108)\r\n\tat org.apache.spark.sql.Dataset.randomSplit(Dataset.scala:2378)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\n"
     ]
    }
   ],
   "source": [
    "# split into training(60%), validation(20%) and test(20%) datasets\n",
    "trainingDf, validationDf, testDf = df.randomSplit([7, 1, 2])\n",
    "\n",
    "#print(trainingDf.take(1))\n",
    "\n",
    "#lets cache these datasets\n",
    "trainingDf.cache()\n",
    "validationDf.cache()\n",
    "testDf.cache()\n",
    "\n",
    "print(\"Num of training observations : %s\" % trainingDf.count())\n",
    "print(\"Num of validation observations : %s\" % validationRdd.count())\n",
    "print(\"Num of test observations : %s\" % testDf.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categoricals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'StringIndexer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-59-1bd0cab4e86e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mpipelineStages\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcolumnName\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcategoricalAttributes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mstringIndexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStringIndexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputCol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolumnName\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputCol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolumnName\u001b[0m\u001b[1;33m+\u001b[0m \u001b[1;34m\"Index\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0mpipelineStages\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstringIndexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0moneHotEncoder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOneHotEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputCol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolumnName\u001b[0m\u001b[1;33m+\u001b[0m \u001b[1;34m\"Index\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputCol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolumnName\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"Vec\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'StringIndexer' is not defined"
     ]
    }
   ],
   "source": [
    "#convert the categorical attributes to binary features\n",
    "categoricalAttributes = ['Origin', 'Dest', 'UniqueCarrier']\n",
    "\n",
    "#Build a list of pipelist stages for the machine learning pipeline. \n",
    "#start by the feature transformer of one hot encoder for building the categorical features\n",
    "pipelineStages = []\n",
    "for columnName in categoricalAttributes:\n",
    "    stringIndexer = StringIndexer(inputCol=columnName, outputCol=columnName+ \"Index\")\n",
    "    pipelineStages.append(stringIndexer)\n",
    "    oneHotEncoder = OneHotEncoder(inputCol=columnName+ \"Index\", outputCol=columnName + \"Vec\")\n",
    "    pipelineStages.append(oneHotEncoder)\n",
    "    \n",
    "    \n",
    "print(\"%s string indexer and one hot encoders transformers\" %  len(pipelineStages) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17 feature columns: ['Year', 'Month', 'DayofMonth', 'DayOfWeek', 'UniqueCarrier', 'CRSElapsedTime', 'ArrDelay', 'DepDelay', 'Origin', 'Dest', 'Distance', 'TaxiOut', 'Season', 'Duration', 'OriginVec', 'DestVec', 'UniqueCarrierVec']\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-61-84390893f3c3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;31m#Build pipeline for feature extraction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[0mfeaturePipeline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstages\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpipelineStages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[0mfeatureOnlyModel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeaturePipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "# Combine all the feature columns into a single column in the dataframe\n",
    "numericColumns = ['Year', 'Month', 'DayofMonth', 'DayOfWeek',\n",
    " 'UniqueCarrier', \n",
    " 'CRSElapsedTime', 'ArrDelay', 'DepDelay',\n",
    " 'Origin', 'Dest', 'Distance', 'TaxiOut', 'Season', 'Duration']\n",
    "\n",
    "categoricalCols = [s + \"Vec\" for s in categoricalAttributes]\n",
    "\n",
    "allFeatureCols =  numericColumns + categoricalCols\n",
    "\n",
    "vectorAssembler = VectorAssembler(\n",
    "    inputCols=allFeatureCols,\n",
    "    outputCol=\"features\")\n",
    "pipelineStages.append(vectorAssembler)\n",
    "\n",
    "print(\"%s feature columns: %s\" % (len(allFeatureCols),allFeatureCols))\n",
    "\n",
    "#Build pipeline for feature extraction\n",
    "featurePipeline = Pipeline(stages=pipelineStages)\n",
    "featureOnlyModel = featurePipeline.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'featureOnlyModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-62-ba71ccc3dc8b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#create list of Dataframes with features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrainingFeaturesDf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeatureOnlyModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mvalidationFeaturesDf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeatureOnlyModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalidationDf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtestFeaturesDf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeatureOnlyModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestDf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'featureOnlyModel' is not defined"
     ]
    }
   ],
   "source": [
    "#create list of Dataframes with features\n",
    "trainingFeaturesDf = featureOnlyModel.transform(df)\n",
    "validationFeaturesDf = featureOnlyModel.transform(validationDf)\n",
    "testFeaturesDf = featureOnlyModel.transform(testDf)\n",
    "\n",
    "#peek\n",
    "trainingFeaturesDf.select(\"features\", \"label\").rdd.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building A Machine Learning Model With Spark ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-63-0ebbac108bb9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# an estimator (classification) (Logistic regression)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaxIter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mregParam\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mlrPipeline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstages\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# Fit the pipeline to create a model from the training data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Configure an machine learning pipeline, which consists of the \n",
    "# an estimator (classification) (Logistic regression)\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.01)\n",
    "lrPipeline = Pipeline(stages=[lr])\n",
    "\n",
    "# Fit the pipeline to create a model from the training data\n",
    "lrPipelineModel = lrPipeline.fit(trainingFeaturesDf)\n",
    "\n",
    "def getAccuracyForPipelineModel(featuresDf, model):\n",
    "    #perform prediction using the featuresdf and pipelineModel\n",
    "    #compute the accuracy in percentage float\n",
    "    results = model.transform(featuresDf)\n",
    "    labelsAndPreds = results.map(lambda p: (p.label, p.prediction))\n",
    "    return (calculateAccuracy(labelsAndPreds), results) \n",
    "\n",
    "# Evaluating the model on training data\n",
    "lrTrainAccuracy, lrTrainResultDf = getAccuracyForPipelineModel(trainingFeaturesDf, lrPipelineModel)\n",
    "\n",
    "# Repeat on test data\n",
    "lrTestAccuracy, lrTestResultDf = getAccuracyForPipelineModel(testFeaturesDf, lrPipelineModel)\n",
    "\n",
    "# Repeat on validation data\n",
    "lrValidationAccuracy, lrValidationResultDf = getAccuracyForPipelineModel(validationFeaturesDf, lrPipelineModel)\n",
    "\n",
    "print(\"==========================================\")\n",
    "print(\"LogisticRegression Model training accuracy (%) = \" + str(lrTrainAccuracy))\n",
    "print(\"LogisticRegression Model test accuracy (%) = \" + str(lrTestAccuracy))\n",
    "print(\"LogisticRegression Model validation accuracy (%) = \" + str(lrValidationAccuracy))\n",
    "print(\"==========================================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyperparameter Tuning with Grid search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxIterRange = [5, 10, 30, 50, 100]\n",
    "regParamRange = [1e-10, 1e-5, 1e-1]\n",
    "#baseline values from previous section\n",
    "bestIter = 10\n",
    "bestRegParam = 0.01\n",
    "bestModel = lr\n",
    "bestAccuracy = lrValidationAccuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#for plotting purpose\n",
    "iterations = []\n",
    "regParams = []\n",
    "accuracies = []\n",
    "for maxIter in maxIterRange:\n",
    "    for rp in regParamRange:\n",
    "        currentLr = LogisticRegression(maxIter=maxIter, regParam=rp)\n",
    "        pipeline = Pipeline(stages=[currentLr])\n",
    "        model = pipeline.fit(trainingFeaturesDf)\n",
    "        \n",
    "        #use validation dataset test for accuracy\n",
    "        accuracy, resultDf = getAccuracyForPipelineModel(validationFeaturesDf, model)\n",
    "        print \"maxIter: %s, regParam: %s, accuracy: %s \" % (maxIter, rp, accuracy)\n",
    "        accuracies.append(accuracy)\n",
    "        regParams.append(log(rp))\n",
    "        iterations.append(maxIter)\n",
    "        \n",
    "        if accuracy > lrValidationAccuracy:\n",
    "            bestIter = maxIter\n",
    "            bestRegParam = rp\n",
    "            bestModel = model\n",
    "            bestAccuracy = accuracy\n",
    "\n",
    "\n",
    "print \"Best parameters: maxIter %s, regParam %s, accuracy : %s\" % (bestIter, bestRegParam, bestAccuracy)\n",
    "\n",
    "# Repeat on test data\n",
    "gridTestAccuracy, gridTestResultDf = getAccuracyForPipelineModel(testFeaturesDf, bestModel)\n",
    "\n",
    "print(\"==========================================\")\n",
    "print(\"Grid search Model test accuracy (%) = \" + str(gridTestAccuracy))\n",
    "print(\"==========================================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "# We use a ParamGridBuilder to construct a grid of parameters to search over.\n",
    "grid = (ParamGridBuilder()\n",
    "        .addGrid(lr.maxIter, maxIterRange) \n",
    "        .addGrid(lr.regParam,regParamRange )\n",
    "        .build())\n",
    "\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "\n",
    "# We now treat the Pipeline as an Estimator, wrapping it in a CrossValidator instance.\n",
    "# A CrossValidator requires an Estimator, a set of Estimator ParamMaps, and an Evaluator.\n",
    "crossValidator = CrossValidator(estimator=lrPipeline, \n",
    "                                estimatorParamMaps=grid, \n",
    "                                numFolds=5,\n",
    "                                evaluator=evaluator)\n",
    "\n",
    "\n",
    "# Run cross-validation, and choose the best model\n",
    "bestCvModel = crossValidator.fit(trainingFeaturesDf)\n",
    "\n",
    "# verify results on training dataset\n",
    "cvTrainAccuracy, cvTrainResultDf = getAccuracyForPipelineModel(trainingFeaturesDf, bestCvModel)\n",
    "\n",
    "# Repeat on test data\n",
    "cvTestAccuracy, cvTestResultDf = getAccuracyForPipelineModel(testFeaturesDf, bestCvModel)\n",
    "\n",
    "print(\"==========================================\")\n",
    "print(\"CV Model training accuracy (%) = \" + str(cvTrainAccuracy))\n",
    "print(\"CV Model test accuracy (%) = \" + str(cvTestAccuracy))\n",
    "print(\"==========================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coefficients for the model\n",
    "linearModel.coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intercept for the model\n",
    "linearModel.intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff_df = pd.DataFrame({\"Feature\": [\"Intercept\"] + featureCols, \"Co-efficients\": np.insert(linearModel.coefficients.toArray(), 0, linearModel.intercept)})\n",
    "coeff_df = coeff_df[[\"Feature\", \"Co-efficients\"]]\n",
    "coeff_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
